{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ICICLE Training Catalog Toggle View Base ICICLE Tapis Software Version: 1.3.0 Category: Category1 Hosted, web-based API for managing data and executing software for research computing Training Tutorials Tapis Pods Service API Version: 1.3.0 Category: Category1 New API providing web-accessible long-lived containers (pods) as-a-service via Kubernetes. Providing WAN-accessible Neo4J, Postgres, and custom-image HTTP pods with a simple API. More templates on the way. Training Tutorials Tapis Pods Service API Version: 1.3.2 Category: Category3 New API providing web-accessible long-lived containers (pods) as-a-service via Kubernetes. Providing WAN-accessible Neo4J, Postgres, and custom-image HTTP pods with a simple API. More templates on the way. Training Tutorials Tapis Pods Service API Version: 1.5.3 Category: Category1 New API providing web-accessible long-lived containers (pods) as-a-service via Kubernetes. Providing WAN-accessible Neo4J, Postgres, and custom-image HTTP pods with a simple API. More templates on the way. Training Tutorials Tapis Pods Service API Version: 1.6.0 Category: Category3 New API providing web-accessible long-lived containers (pods) as-a-service via Kubernetes. Providing WAN-accessible Neo4J, Postgres, and custom-image HTTP pods with a simple API. More templates on the way. Training Tutorials PPOD Core Version: 0.5.0 Category: Category3 A LinkML schema describing the core elements of PPOD (Person-Project-Organization-Dataset) information. Training Tutorials function toggleView() { var container = document.getElementById('cards-container'); if (container.classList.contains('list-view')) { container.classList.remove('list-view'); container.classList.add('grid-view'); } else { container.classList.remove('grid-view'); container.classList.add('list-view'); } }","title":"ICICLE Training Catalog"},{"location":"#icicle-training-catalog","text":"Toggle View","title":"ICICLE Training Catalog"},{"location":"Base_ICICLE_Tapis_Software/","text":"Redirecting... You are being redirected. If not, click here . window.onload = function() { window.open(\"https://tapis.readthedocs.io/en/latest/index.html\", \"_blank\"); }","title":"Base ICICLE Tapis Software"},{"location":"CI_Components_Catalog/","text":"Redirecting... You are being redirected. If not, click here . window.onload = function() { window.open(\"https://components.pods.icicle.tapis.io/data\"); }","title":"CI Components Catalog"},{"location":"Event_Engine/","text":"Redirecting... You are being redirected. If not, click here . window.onload = function() { window.open(\"https://crates.io/crates/event-engine\", \"_blank\"); }","title":"Event Engine"},{"location":"Smart_Foodsheds_Visual_Analytics_Dashboard/","text":"Redirecting... You are being redirected. If not, click here . window.onload = function() { window.open(\"https://vaapifrontend.pods.icicle.tapis.io/#/\", \"_blank\"); }","title":"Smart Foodsheds Visual Analytics Dashboard"},{"location":"SoftwarePilot/","text":"Redirecting... You are being redirected. If not, click here . window.onload = function() { window.open(\"https://pypi.org/project/SoftwarePilot/\"); }","title":"SoftwarePilot"},{"location":"Tapis_Pods_Service/","text":"Redirecting... You are being redirected. If not, click here . window.onload = function() { window.open(\"https://tapis.readthedocs.io/en/latest/technical/pods.html\", \"_blank\"); }","title":"Tapis Pods Service"},{"location":"PPODLinkML/","text":"PPOD_FSL - A LinkML Schema for UC Davis Food Systems Lab Applications This repository contains a LinkML schema for a version of the PPOD (Persons-Projects-Organizations-Datasets) data pattern that describes resources being cataloged by the UC Davis Food Systems Lab . These are resources pertinent to California foodsheds and conservation activities, and include lists of organizations, people, programs, projects, tools, datasets, and guidelines and mandates. These resources are currently maintained in a Google Sheets document which is converted into a RDF Turtle file using a Python script that is posted in the PPODtottl repository . LinkML is an emerging standard and toolset for describing data schemas with an orientation towards building linked data applications. LinkML data schemas are written in YAML and the framework provides tools to convert these schemas into a number of other formats, including JSON-Schema, OWL, SQL DDL, SHACL, ShEx, and Python classes. The framework also provides tools for validating and converting data between different formats including RDF, CSV, JSON, YAML, and SQLite databases. As such LinkML is intended as a lingua franca for interoperability between data schemas and datasets. This repository contains two main files, PPOD.yaml and vocabs.yaml . PPOD.yaml contains the classes and slots describing the resources in the Google Sheets document and the RDF file that it generates, and vocabs.yaml gives the enumerated vocabularies that are being used. These enumerated vocabularies include categories such as types of organizations and lists of sustainability issues, ecoregions, and habitat types. LinkML provides a facility to automatically generate a set of web pages describing the elements in the schema, one web page per element. This has been carried out for the PPOD FSL schema and has been posted on GitHub Pages . Further work on PPOD_FSL will include providing standardized identifiers for newly created data schema elements and instance data using the URL resolution service w3id.org . Over the near future, work on PPOD schemas will shift to modularizing PPOD into a PPOD core and individual PPOD application schemas that inherit classes and slots from PPOD-core. Acknowledgements Work on the PPOD FSL LinkML schema has been funded under the NSF ICICLE AI Institute , grant number OAC-2112606.","title":"PPOD_FSL - A LinkML Schema for UC Davis Food Systems Lab Applications"},{"location":"PPODLinkML/#ppod_fsl-a-linkml-schema-for-uc-davis-food-systems-lab-applications","text":"This repository contains a LinkML schema for a version of the PPOD (Persons-Projects-Organizations-Datasets) data pattern that describes resources being cataloged by the UC Davis Food Systems Lab . These are resources pertinent to California foodsheds and conservation activities, and include lists of organizations, people, programs, projects, tools, datasets, and guidelines and mandates. These resources are currently maintained in a Google Sheets document which is converted into a RDF Turtle file using a Python script that is posted in the PPODtottl repository . LinkML is an emerging standard and toolset for describing data schemas with an orientation towards building linked data applications. LinkML data schemas are written in YAML and the framework provides tools to convert these schemas into a number of other formats, including JSON-Schema, OWL, SQL DDL, SHACL, ShEx, and Python classes. The framework also provides tools for validating and converting data between different formats including RDF, CSV, JSON, YAML, and SQLite databases. As such LinkML is intended as a lingua franca for interoperability between data schemas and datasets. This repository contains two main files, PPOD.yaml and vocabs.yaml . PPOD.yaml contains the classes and slots describing the resources in the Google Sheets document and the RDF file that it generates, and vocabs.yaml gives the enumerated vocabularies that are being used. These enumerated vocabularies include categories such as types of organizations and lists of sustainability issues, ecoregions, and habitat types. LinkML provides a facility to automatically generate a set of web pages describing the elements in the schema, one web page per element. This has been carried out for the PPOD FSL schema and has been posted on GitHub Pages . Further work on PPOD_FSL will include providing standardized identifiers for newly created data schema elements and instance data using the URL resolution service w3id.org . Over the near future, work on PPOD schemas will shift to modularizing PPOD into a PPOD core and individual PPOD application schemas that inherit classes and slots from PPOD-core.","title":"PPOD_FSL - A LinkML Schema for UC Davis Food Systems Lab Applications"},{"location":"PPODLinkML/#acknowledgements","text":"Work on the PPOD FSL LinkML schema has been funded under the NSF ICICLE AI Institute , grant number OAC-2112606.","title":"Acknowledgements"},{"location":"PPODLinkML/LICENSE/","text":"Creative Commons Legal Code CC0 1.0 Universal CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE LEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN ATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS INFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES REGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED HEREUNDER. Statement of Purpose The laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an \"owner\") of an original work of authorship and/or a database (each, a \"Work\"). Certain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works (\"Commons\") that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others. For these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the \"Affirmer\"), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights. Copyright and Related Rights. A Work made available under CC0 may be protected by copyright and related or neighboring rights (\"Copyright and Related Rights\"). Copyright and Related Rights include, but are not limited to, the following: i. the right to reproduce, adapt, distribute, perform, display, communicate, and translate a Work; ii. moral rights retained by the original author(s) and/or performer(s); iii. publicity and privacy rights pertaining to a person's image or likeness depicted in a Work; iv. rights protecting against unfair competition in regards to a Work, subject to the limitations in paragraph 4(a), below; v. rights protecting the extraction, dissemination, use and reuse of data in a Work; vi. database rights (such as those arising under Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, and under any national implementation thereof, including any amended or successor version of such directive); and vii. other similar, equivalent or corresponding rights throughout the world based on applicable law or treaty, and any national implementations thereof. Waiver. To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer's Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the \"Waiver\"). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer's heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer's express Statement of Purpose. Public License Fallback. Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer's express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer's Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the \"License\"). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer's express Statement of Purpose. Limitations and Disclaimers. a. No trademark or patent rights held by Affirmer are waived, abandoned, surrendered, licensed or otherwise affected by this document. b. Affirmer offers the Work as-is and makes no representations or warranties of any kind concerning the Work, express, implied, statutory or otherwise, including without limitation warranties of title, merchantability, fitness for a particular purpose, non infringement, or the absence of latent or other defects, accuracy, or the present or absence of errors, whether or not discoverable, all to the greatest extent permissible under applicable law. c. Affirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person's Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work. d. Affirmer understands and acknowledges that Creative Commons is not a party to this document and has no duty or obligation with respect to this CC0 or use of the Work.","title":"LICENSE"},{"location":"camera-traps/","text":"camera-traps The camera-traps application is both a simulator and an edge device application for classifying images, with the first deployment specializing in wildlife images. The simulation environment will be implemented first and serve as a test bed for protocols and techniques that optimize storage, execution time, power and accuracy. The ultimate goal is to deploy a version of this application on camera-trap devices in the wild. Architectual Overview This application uses the event-engine library to implement its plugin architecture and event-driven communication. The engine uses zmq sockets to deliver events between senders and the subscribers interested in specific events. The event-engine supports internal and external plugins. Internal plugins are Rust plugins delivered with camera-traps and run in the camera-traps process. External plugins are configured by camera-traps to run outside the camera-traps process and use a TCP port to send and receive events. By using TCP, external plugins can be written in any language that supports the flatbuffers wire protocol. Quick Start To quickly start the application under Docker using docker-compose, follow these steps: cd releases Follow the directions in the README.md file Application Configuration The camera-traps application requires configuration through environment variables or configuration files. When launching the application from a releases subdirectory, the specific release's config directory will contain the default configuration files for running a short simulation test. In general, plugins can also depend on their own environment variables and/or configuration files, and the same is true of test programs. The releases directory contains docker-compose files that use default configurations, which can serve as a template for production environment configuration. Target Environment Variable Default File Notes camera-traps application TRAPS_CONFIG_FILE ~/traps.toml Can be 1st command line parameter image_gen_plugin /input.json image_store_plugin TRAPS_IMAGE_STORE_FILE ~/traps-image-store.toml power_measure_plugin TRAPS_POWER_LOG_PATH ~/logs oracle_monitor_plugin TRAPS_ORACLE_OUTPUT_PATH ~/output integration tests TRAPS_INTEGRATION_CONFIG_FILE ~/traps-integration.toml logger TRAPS_LOG4RS_CONFIG_FILE resources/log4rs.yml Packaged with application The external python plugins run in their own processes and do not currently use environment variables. The camera-traps application uses log4rs as its log manager. The log settings in resources/log4rs.yml source code will be used unless overridden by assigning a log4rs.yml configuration filepath to the TRAPS_LOG4RS_CONFIG_FILE environment variable. To maximize logging, set root level to trace in the effective log4rs.yml file. Also, include the observer_plugin in the internal plugins list in the effective traps.toml file. Plugin Configuration Camera-traps uses a TOML file to configure the internal and external plugins it loads. Internal plugins are registered with the event-engine by simply specfying their names since their runtime characteristics are compiled into the application. External plugins, on the other hand, require more detailed information in order to be registered. Here is the example resources/traps.toml file content: # This is the camera-traps application configuration file for versions 0.x.y of the application. # It assumes the use of containers and docker-compose as the deployment mechanism. title = \"Camera-Traps Application Configuration v0.3.2\" # The event engine's publish and subscribe port used to create the event_engine::App instance. publish_port = 5559 subscribe_port = 5560 # An absolute path to the image directory is required but a file name prefix is optional. # If present the prefix is preprended to generated image file names. This is the directory # into which the image_recv_plugin writes incoming images and the image_store_plugin may # delete images or output the scores for images. images_output_dir = \"/root/camera-traps/images\" # image_file_prefix = \"\" # The container for both internal and external plugins. Internal plugins are written in rust # and compiled into the camera-traps application. External plugins are usually written in # python but can be written in any language. External plugins run in their own processes # and communicate via tcp or ipc. [plugins] # Uncomment the internal plugins loaded when the camera-traps application starts. internal = [ # \"image_gen_plugin\", \"image_recv_plugin\", # \"image_score_plugin\", \"image_store_plugin\", # \"observer_plugin\" ] # Configure each of the active internal plugins with the image processing action they should # take when new work is received. If no action is specified for a plugin, its no-op action # is used by default. internal_actions = [ \"image_recv_write_file_action\", \"image_store_file_action\" ] # External plugins require more configuration information than internal plugins. # Each plugin must subscribe to PluginTerminateEvent. # # Note that each plugin must specify the external port to use in TWO PLACES: here as well as # in the docker-compose.yml file. If external_port changes here, it must ALSO be changed in the # docker-compose.yml file. [[plugins.external]] plugin_name = \"ext_image_gen_plugin\" id = \"d3266646-41ec-11ed-a96f-5391348bab46\" external_port = 6000 subscriptions = [ \"PluginTerminateEvent\" ] [[plugins.external]] plugin_name = \"ext_image_score_plugin\" id = \"d6e8e42a-41ec-11ed-a36f-a3dcc1cc761a\" external_port = 6001 subscriptions = [ \"ImageReceivedEvent\", \"PluginTerminateEvent\" ] [[plugins.external]] plugin_name = \"ext_power_monitor_plugin\" id = \"4a0fca25-1935-472a-8674-58f22c3a32b3\" external_port = 6010 subscriptions = [ \"MonitorPowerStartEvent\", \"MonitorPowerStopEvent\", \"PluginTerminateEvent\" ] [[plugins.external]] plugin_name = \"ext_power_control_plugin\" id = \"a59621f2-4db6-4892-bda1-59ecb7ff24ae\" external_port = 6011 subscriptions = [ \"PluginTerminateEvent\" ] [[plugins.external]] plugin_name = \"ext_oracle_monitor_plugin\" id = \"6e153711-9823-4ee6-b608-58e2e801db51\" external_port = 6011 subscriptions = [ \"ImageScoredEvent\", \"ImageStoredEvent\", \"ImageDeletedEvent\", \"PluginTerminateEvent\" ] Every plugin must subscribe to the PluginTerminateEvent, which upon receipt causes the plugin to stop. Subscriptions are statically defined in internal plugin code and explicitly configured for external plugins. External plugins also provide their predetermined UUIDs and external TCP ports. Camera-traps looks for its configuration file using these methods in the order shown: The environment variable $TRAPS_CONFIG_FILE. The first command line argument. $HOME/traps.toml The first file it finds it uses. If no configuration file is found the program aborts. Internal Plugin Configuration The names listed in the internal list are the rust plugin file names. These plugins run as separate threads in the camera-traps process. The internal_actions list contains the file names that implement the different algorithms or actions associated with each internal plugin. A naming convention is used to associate actions with their plugins: An action name starts with its plugin name minus the trailing \"plugin\" part, followed by an action identifier part, and ends with \"_action\". Each plugin has a no-op action that causes it to take no action other than, possibly, generating the next event in the pipeline. For example, image_gen_noop_action is associated with the image_gen_plugin . Internal plugins for which no corresponding action is specified are assigned their no-op plugin by default. image_recv_plugin When image_recv_write_file_action is specifed, the image_recv_plugin uses the image_dir and image_file_prefix parameters to manage files. The image_dir is the directory into which image files are placed. Image file names are constructed from the information received in a NewImageEvent and have this format: . The image_uuid and image_format are from the NewImageEvent. The image_file_prefix can be the empty string and the image_format is always lowercased when used in the file name. Support for NVIDIA The Image Scoring plugin can make use of NVIDIA GPUs to improve the performance of object detection and classification with some ML models. In order to make use of NVIDIA GPUs in the Camera Traps application, the following steps must be taken: Ensure the NVIDIA drivers are installed natively on the machine . For example, on Ubuntu LTS, follow the instructions in Section 3.1 here . Be sure to reboot your machine after adding the keyring and installing the drivers. You can check to see if the drivers are installed properly and communicating with the hardware by running the following command: nvidia-smi Install the NVIDIA Container Toolkit and configure the Docker Runtime . See the instructions here . Make sure to restart Docker after installing and configuring the toolkit. To check if the toolkit and Docker are installed and configured correctly, run the following: docker run --gpus=all --rm -it ubuntu nvidia-smi The output should be similar to the output from Step 1. Update the Camera Traps Compose File to Use GPUs . Starting with release 0.3.3, the official Camera Traps releases docker-compose files include stanzas for making NVIDIA GPUs available to both the Image Scoring and Power Monitoring plugins. At this time, those stanzas must be uncommented; see the docker-compose.yml file for more details. Support for ARM CPUs This section is a work in progress... We are currently working on support for ARM CPUs, including support for Mac OSX M* hardware. Developer Information Using Flatbuffers In-memory representations of events are translated into flatbuffer binary streams plus a leading two byte sequence that identifies the event type. These statically defined byte sequences are specified in the events.rs source file and repeated here for convenience. // Each event is assigned a binary prefix that zqm uses to route incoming binary streams to all of the event's subscribers. pub const NEW_IMAGE_PREFIX: [u8; 2] = [0x01, 0x00]; pub const IMAGE_RECEIVED_PREFIX: [u8; 2] = [0x02, 0x00]; pub const IMAGE_SCORED_PREFIX: [u8; 2] = [0x03, 0x00]; pub const IMAGE_STORED_PREFIX: [u8; 2] = [0x04, 0x00]; pub const IMAGE_DELETED_PREFIX: [u8; 2] = [0x05, 0x00]; pub const PLUGIN_STARTED_PREFIX: [u8; 2] = [0x10, 0x00]; pub const PLUGIN_TERMINATING_PREFIX: [u8; 2] = [0x11, 0x00]; pub const PLUGIN_TERMINATE_PREFIX: [u8; 2] = [0x12, 0x00]; pub const MONITOR_POWER_START_PREFIX: [u8; 2] = [0x20, 0x00]; pub const MONITOR_POWER_STOP_PREFIX: [u8; 2] = [0x21, 0x00]; Each event sent or received begins with its two byte prefix followed by its serialized form as defined in the camera-traps flatbuffer definition file ( events.fbs ). The following section describes how to generate Rust source code from this definition file, a similar process can be used for any language supported by flatbuffers. Updating the flatbuffers messages Flatbuffers info: https://google.github.io/flatbuffers/ The flatbuffers messages schema is defined in the resources/events.fsb file. To change the message formats do the following: Edit the resources/events.fsb file with your changes. From the camera-traps directory, regenerate the events_generated.rs code with the command: $ flatc --rust -o src resources/events.fbs (Optional) Add the following line to the top of the src/events_generated.rs file so that clippy warnings are suppressed: // this line added to keep clippy happy #![allow(clippy::all)] Plugin Start and Stop Protocol Each plugin is required to conform to the following conventions: Register for the PluginTerminateEvent . Send a PluginStartedEvent when it begins executing. Send a PluginTerminatingEvent when it shuts down. The PluginStartedEvent advertises a plugin's name and uuid when it starts. When a plugin receives a PluginTerminateEvent , it checks if the event's target_plugin_name matches its name or the wildcard name ( ). If either is true, then the plugin is expected to gracefully terminate. The plugin is also expected to gracefully terminate if the event's target_plugin_uuid matches the plugin's uuid. Part of plugin termination is for it to send a PluginTerminatingEvent to advertise that it's shutting down, whether in response to a PluginTerminateEvent* or for any other reason. Building and Running under Docker The instructions in this section assume Docker (and docker-compose) are installed, as well as Rust , cargo and make. From the top-level camera-traps directory, issue the following command to build the application's Docker images: make build See Makefile for details. From the releases directory, navigate to the subdirectory of the specific release you want to run. Issue the following command to run the application, including the external plugins for which it's configured: docker-compose up See docker-compose.yaml for details. From the same release directory, issue the following command to stop the application: docker-compose down Building and Running the Rust Code If you're just interested in building the Rust, issue cargo build from the top-level camera-traps directory. Alternatively, issue cargo run to build and run it. External plugins are not started using this approach. The internal plugins and their actions are configured using a traps.toml file, as discussed above. Integration Testing The camera-traps/tests directory contains integration_tests.rs program. The integration test program runs as an external plugin configured via a traps.toml file as shown above. See the top-level comments in the source code for details. Plugin Development This section addresses two questions: Why would I want to create a plugin? What kind of plugin should I create? One would want to create their own plugin if they wanted to read or write events and perform some new action that isn't currently implemented. If an existing plugin doesn't do what you want, you have the option of modifying that plugin or creating another plugin that acts on the same events and does what you need. For example, the image_gen_plugin injects new images into the event stream, the image_recv_plugin writes new images to file, etc. The observer_plugin is one that subscribes to all events and logs them for debugging purposes. Most of the time we don't run the observer_plugin , but if we want extended logging we just include it to run in the traps.toml file. In this case, having a separate plugin from which we can customize the logging of all events is more convenient then adding that logging capability to each existing plugin. Another reason for introducing a new plugin would be to also service new events. As the application evolves new capabilities might require new events. This occurred as we develop support for power monitoring, which introduces 2 new events and a plugin to handle them. When implementing a plugin the choice between internal and external is often technology driven. Do we want to write a plugin in Rust and compile it into the application (internal) or do we want to write it in some other language and start it up in its own container (external)? Considerations as to which approach to take include performance, resource usage, and availability of domain-specific libraries. Release Procedures When development on a new release begins a launch configuration is created in the new release's own releases subdirectory. When development completes and the final version of the release's images are pushed to docker hub, we tag those images with the release number and with the \"latest\" tag. To be able to rebuild a release at anytime, we also tag the release's source code in github. The tag is the same as the release version number. Once confident that the tagged code is stable, release tags can be protected using github tag protection . Acknowledgements This work has been funded by grants from the National Science Foundation, including the ICICLE AI Institute (OAC 2112606) and Tapis (OAC 1931439).","title":"camera-traps"},{"location":"camera-traps/#camera-traps","text":"The camera-traps application is both a simulator and an edge device application for classifying images, with the first deployment specializing in wildlife images. The simulation environment will be implemented first and serve as a test bed for protocols and techniques that optimize storage, execution time, power and accuracy. The ultimate goal is to deploy a version of this application on camera-trap devices in the wild.","title":"camera-traps"},{"location":"camera-traps/#architectual-overview","text":"This application uses the event-engine library to implement its plugin architecture and event-driven communication. The engine uses zmq sockets to deliver events between senders and the subscribers interested in specific events. The event-engine supports internal and external plugins. Internal plugins are Rust plugins delivered with camera-traps and run in the camera-traps process. External plugins are configured by camera-traps to run outside the camera-traps process and use a TCP port to send and receive events. By using TCP, external plugins can be written in any language that supports the flatbuffers wire protocol.","title":"Architectual Overview"},{"location":"camera-traps/#quick-start","text":"To quickly start the application under Docker using docker-compose, follow these steps: cd releases Follow the directions in the README.md file","title":"Quick Start"},{"location":"camera-traps/#application-configuration","text":"The camera-traps application requires configuration through environment variables or configuration files. When launching the application from a releases subdirectory, the specific release's config directory will contain the default configuration files for running a short simulation test. In general, plugins can also depend on their own environment variables and/or configuration files, and the same is true of test programs. The releases directory contains docker-compose files that use default configurations, which can serve as a template for production environment configuration. Target Environment Variable Default File Notes camera-traps application TRAPS_CONFIG_FILE ~/traps.toml Can be 1st command line parameter image_gen_plugin /input.json image_store_plugin TRAPS_IMAGE_STORE_FILE ~/traps-image-store.toml power_measure_plugin TRAPS_POWER_LOG_PATH ~/logs oracle_monitor_plugin TRAPS_ORACLE_OUTPUT_PATH ~/output integration tests TRAPS_INTEGRATION_CONFIG_FILE ~/traps-integration.toml logger TRAPS_LOG4RS_CONFIG_FILE resources/log4rs.yml Packaged with application The external python plugins run in their own processes and do not currently use environment variables. The camera-traps application uses log4rs as its log manager. The log settings in resources/log4rs.yml source code will be used unless overridden by assigning a log4rs.yml configuration filepath to the TRAPS_LOG4RS_CONFIG_FILE environment variable. To maximize logging, set root level to trace in the effective log4rs.yml file. Also, include the observer_plugin in the internal plugins list in the effective traps.toml file.","title":"Application Configuration"},{"location":"camera-traps/#plugin-configuration","text":"Camera-traps uses a TOML file to configure the internal and external plugins it loads. Internal plugins are registered with the event-engine by simply specfying their names since their runtime characteristics are compiled into the application. External plugins, on the other hand, require more detailed information in order to be registered. Here is the example resources/traps.toml file content: # This is the camera-traps application configuration file for versions 0.x.y of the application. # It assumes the use of containers and docker-compose as the deployment mechanism. title = \"Camera-Traps Application Configuration v0.3.2\" # The event engine's publish and subscribe port used to create the event_engine::App instance. publish_port = 5559 subscribe_port = 5560 # An absolute path to the image directory is required but a file name prefix is optional. # If present the prefix is preprended to generated image file names. This is the directory # into which the image_recv_plugin writes incoming images and the image_store_plugin may # delete images or output the scores for images. images_output_dir = \"/root/camera-traps/images\" # image_file_prefix = \"\" # The container for both internal and external plugins. Internal plugins are written in rust # and compiled into the camera-traps application. External plugins are usually written in # python but can be written in any language. External plugins run in their own processes # and communicate via tcp or ipc. [plugins] # Uncomment the internal plugins loaded when the camera-traps application starts. internal = [ # \"image_gen_plugin\", \"image_recv_plugin\", # \"image_score_plugin\", \"image_store_plugin\", # \"observer_plugin\" ] # Configure each of the active internal plugins with the image processing action they should # take when new work is received. If no action is specified for a plugin, its no-op action # is used by default. internal_actions = [ \"image_recv_write_file_action\", \"image_store_file_action\" ] # External plugins require more configuration information than internal plugins. # Each plugin must subscribe to PluginTerminateEvent. # # Note that each plugin must specify the external port to use in TWO PLACES: here as well as # in the docker-compose.yml file. If external_port changes here, it must ALSO be changed in the # docker-compose.yml file. [[plugins.external]] plugin_name = \"ext_image_gen_plugin\" id = \"d3266646-41ec-11ed-a96f-5391348bab46\" external_port = 6000 subscriptions = [ \"PluginTerminateEvent\" ] [[plugins.external]] plugin_name = \"ext_image_score_plugin\" id = \"d6e8e42a-41ec-11ed-a36f-a3dcc1cc761a\" external_port = 6001 subscriptions = [ \"ImageReceivedEvent\", \"PluginTerminateEvent\" ] [[plugins.external]] plugin_name = \"ext_power_monitor_plugin\" id = \"4a0fca25-1935-472a-8674-58f22c3a32b3\" external_port = 6010 subscriptions = [ \"MonitorPowerStartEvent\", \"MonitorPowerStopEvent\", \"PluginTerminateEvent\" ] [[plugins.external]] plugin_name = \"ext_power_control_plugin\" id = \"a59621f2-4db6-4892-bda1-59ecb7ff24ae\" external_port = 6011 subscriptions = [ \"PluginTerminateEvent\" ] [[plugins.external]] plugin_name = \"ext_oracle_monitor_plugin\" id = \"6e153711-9823-4ee6-b608-58e2e801db51\" external_port = 6011 subscriptions = [ \"ImageScoredEvent\", \"ImageStoredEvent\", \"ImageDeletedEvent\", \"PluginTerminateEvent\" ] Every plugin must subscribe to the PluginTerminateEvent, which upon receipt causes the plugin to stop. Subscriptions are statically defined in internal plugin code and explicitly configured for external plugins. External plugins also provide their predetermined UUIDs and external TCP ports. Camera-traps looks for its configuration file using these methods in the order shown: The environment variable $TRAPS_CONFIG_FILE. The first command line argument. $HOME/traps.toml The first file it finds it uses. If no configuration file is found the program aborts.","title":"Plugin Configuration"},{"location":"camera-traps/#internal-plugin-configuration","text":"The names listed in the internal list are the rust plugin file names. These plugins run as separate threads in the camera-traps process. The internal_actions list contains the file names that implement the different algorithms or actions associated with each internal plugin. A naming convention is used to associate actions with their plugins: An action name starts with its plugin name minus the trailing \"plugin\" part, followed by an action identifier part, and ends with \"_action\". Each plugin has a no-op action that causes it to take no action other than, possibly, generating the next event in the pipeline. For example, image_gen_noop_action is associated with the image_gen_plugin . Internal plugins for which no corresponding action is specified are assigned their no-op plugin by default.","title":"Internal Plugin Configuration"},{"location":"camera-traps/#image_recv_plugin","text":"When image_recv_write_file_action is specifed, the image_recv_plugin uses the image_dir and image_file_prefix parameters to manage files. The image_dir is the directory into which image files are placed. Image file names are constructed from the information received in a NewImageEvent and have this format: . The image_uuid and image_format are from the NewImageEvent. The image_file_prefix can be the empty string and the image_format is always lowercased when used in the file name.","title":"image_recv_plugin"},{"location":"camera-traps/#support-for-nvidia","text":"The Image Scoring plugin can make use of NVIDIA GPUs to improve the performance of object detection and classification with some ML models. In order to make use of NVIDIA GPUs in the Camera Traps application, the following steps must be taken: Ensure the NVIDIA drivers are installed natively on the machine . For example, on Ubuntu LTS, follow the instructions in Section 3.1 here . Be sure to reboot your machine after adding the keyring and installing the drivers. You can check to see if the drivers are installed properly and communicating with the hardware by running the following command: nvidia-smi Install the NVIDIA Container Toolkit and configure the Docker Runtime . See the instructions here . Make sure to restart Docker after installing and configuring the toolkit. To check if the toolkit and Docker are installed and configured correctly, run the following: docker run --gpus=all --rm -it ubuntu nvidia-smi The output should be similar to the output from Step 1. Update the Camera Traps Compose File to Use GPUs . Starting with release 0.3.3, the official Camera Traps releases docker-compose files include stanzas for making NVIDIA GPUs available to both the Image Scoring and Power Monitoring plugins. At this time, those stanzas must be uncommented; see the docker-compose.yml file for more details.","title":"Support for NVIDIA"},{"location":"camera-traps/#support-for-arm-cpus","text":"This section is a work in progress... We are currently working on support for ARM CPUs, including support for Mac OSX M* hardware.","title":"Support for ARM CPUs"},{"location":"camera-traps/#developer-information","text":"","title":"Developer Information"},{"location":"camera-traps/#using-flatbuffers","text":"In-memory representations of events are translated into flatbuffer binary streams plus a leading two byte sequence that identifies the event type. These statically defined byte sequences are specified in the events.rs source file and repeated here for convenience. // Each event is assigned a binary prefix that zqm uses to route incoming binary streams to all of the event's subscribers. pub const NEW_IMAGE_PREFIX: [u8; 2] = [0x01, 0x00]; pub const IMAGE_RECEIVED_PREFIX: [u8; 2] = [0x02, 0x00]; pub const IMAGE_SCORED_PREFIX: [u8; 2] = [0x03, 0x00]; pub const IMAGE_STORED_PREFIX: [u8; 2] = [0x04, 0x00]; pub const IMAGE_DELETED_PREFIX: [u8; 2] = [0x05, 0x00]; pub const PLUGIN_STARTED_PREFIX: [u8; 2] = [0x10, 0x00]; pub const PLUGIN_TERMINATING_PREFIX: [u8; 2] = [0x11, 0x00]; pub const PLUGIN_TERMINATE_PREFIX: [u8; 2] = [0x12, 0x00]; pub const MONITOR_POWER_START_PREFIX: [u8; 2] = [0x20, 0x00]; pub const MONITOR_POWER_STOP_PREFIX: [u8; 2] = [0x21, 0x00]; Each event sent or received begins with its two byte prefix followed by its serialized form as defined in the camera-traps flatbuffer definition file ( events.fbs ). The following section describes how to generate Rust source code from this definition file, a similar process can be used for any language supported by flatbuffers.","title":"Using Flatbuffers"},{"location":"camera-traps/#updating-the-flatbuffers-messages","text":"Flatbuffers info: https://google.github.io/flatbuffers/ The flatbuffers messages schema is defined in the resources/events.fsb file. To change the message formats do the following: Edit the resources/events.fsb file with your changes. From the camera-traps directory, regenerate the events_generated.rs code with the command: $ flatc --rust -o src resources/events.fbs (Optional) Add the following line to the top of the src/events_generated.rs file so that clippy warnings are suppressed: // this line added to keep clippy happy #![allow(clippy::all)]","title":"Updating the flatbuffers messages"},{"location":"camera-traps/#plugin-start-and-stop-protocol","text":"Each plugin is required to conform to the following conventions: Register for the PluginTerminateEvent . Send a PluginStartedEvent when it begins executing. Send a PluginTerminatingEvent when it shuts down. The PluginStartedEvent advertises a plugin's name and uuid when it starts. When a plugin receives a PluginTerminateEvent , it checks if the event's target_plugin_name matches its name or the wildcard name ( ). If either is true, then the plugin is expected to gracefully terminate. The plugin is also expected to gracefully terminate if the event's target_plugin_uuid matches the plugin's uuid. Part of plugin termination is for it to send a PluginTerminatingEvent to advertise that it's shutting down, whether in response to a PluginTerminateEvent* or for any other reason.","title":"Plugin Start and Stop Protocol"},{"location":"camera-traps/#building-and-running-under-docker","text":"The instructions in this section assume Docker (and docker-compose) are installed, as well as Rust , cargo and make. From the top-level camera-traps directory, issue the following command to build the application's Docker images: make build See Makefile for details. From the releases directory, navigate to the subdirectory of the specific release you want to run. Issue the following command to run the application, including the external plugins for which it's configured: docker-compose up See docker-compose.yaml for details. From the same release directory, issue the following command to stop the application: docker-compose down","title":"Building and Running under Docker"},{"location":"camera-traps/#building-and-running-the-rust-code","text":"If you're just interested in building the Rust, issue cargo build from the top-level camera-traps directory. Alternatively, issue cargo run to build and run it. External plugins are not started using this approach. The internal plugins and their actions are configured using a traps.toml file, as discussed above.","title":"Building and Running the Rust Code"},{"location":"camera-traps/#integration-testing","text":"The camera-traps/tests directory contains integration_tests.rs program. The integration test program runs as an external plugin configured via a traps.toml file as shown above. See the top-level comments in the source code for details.","title":"Integration Testing"},{"location":"camera-traps/#plugin-development","text":"This section addresses two questions: Why would I want to create a plugin? What kind of plugin should I create? One would want to create their own plugin if they wanted to read or write events and perform some new action that isn't currently implemented. If an existing plugin doesn't do what you want, you have the option of modifying that plugin or creating another plugin that acts on the same events and does what you need. For example, the image_gen_plugin injects new images into the event stream, the image_recv_plugin writes new images to file, etc. The observer_plugin is one that subscribes to all events and logs them for debugging purposes. Most of the time we don't run the observer_plugin , but if we want extended logging we just include it to run in the traps.toml file. In this case, having a separate plugin from which we can customize the logging of all events is more convenient then adding that logging capability to each existing plugin. Another reason for introducing a new plugin would be to also service new events. As the application evolves new capabilities might require new events. This occurred as we develop support for power monitoring, which introduces 2 new events and a plugin to handle them. When implementing a plugin the choice between internal and external is often technology driven. Do we want to write a plugin in Rust and compile it into the application (internal) or do we want to write it in some other language and start it up in its own container (external)? Considerations as to which approach to take include performance, resource usage, and availability of domain-specific libraries.","title":"Plugin Development"},{"location":"camera-traps/#release-procedures","text":"When development on a new release begins a launch configuration is created in the new release's own releases subdirectory. When development completes and the final version of the release's images are pushed to docker hub, we tag those images with the release number and with the \"latest\" tag. To be able to rebuild a release at anytime, we also tag the release's source code in github. The tag is the same as the release version number. Once confident that the tagged code is stable, release tags can be protected using github tag protection .","title":"Release Procedures"},{"location":"camera-traps/#acknowledgements","text":"This work has been funded by grants from the National Science Foundation, including the ICICLE AI Institute (OAC 2112606) and Tapis (OAC 1931439).","title":"Acknowledgements"},{"location":"camera-traps/RELEASE_NOTES/","text":"Camera Traps Release Notes Version 0.3.3 Implementation of power monitoring plugin for x86 and ARM architectures. Implementation of oracle plugin to analyze simulation results. Various bug fixes. Version 0.3.2 Runtime configuration updates, principally involving changes to the releases directory structure. Improved release procedure documentation in top-level README file. Version 0.3.1 Interum release with Rust support for 2 new power monitoring events. Removal of image_uuid field from ImageLabelScore type used in ImageScoredEvent. MonitorPowerStartEvent and MonitorPowerStopEvent implemented in Rust (Python support in progress). The image_store_plugin deletes files of all types associated with an image when that image is deleted. Version 0.3.0 Initial release of camera-traps images with the following features: Event Engine v0.2.0. Internal Rust production plugins image_recv_plugin, image_store_plugin and observer_plugin. The image_recv_plugin always writes the image to the configured images directory. The image_store_plugin determines if an images, based on its score, should be kept or deleted. If kept, its scores are written to a similarly named file with a .score suffix. Internal Rust test plugins image_gen_plugin and image_score_plugin. These plugins can be used when not running the cooresponding external plugins. External Python production plugins image_gen_plugin and image_score_plugin. The image_gen_plugin injects serveral built-in images into the application and image_score_plugin calls the Microsoft detector with its MegaDetector model to score the images for animal content.","title":"Camera Traps Release Notes"},{"location":"camera-traps/RELEASE_NOTES/#camera-traps-release-notes","text":"","title":"Camera Traps Release Notes"},{"location":"camera-traps/RELEASE_NOTES/#version-033","text":"Implementation of power monitoring plugin for x86 and ARM architectures. Implementation of oracle plugin to analyze simulation results. Various bug fixes.","title":"Version 0.3.3"},{"location":"camera-traps/RELEASE_NOTES/#version-032","text":"Runtime configuration updates, principally involving changes to the releases directory structure. Improved release procedure documentation in top-level README file.","title":"Version 0.3.2"},{"location":"camera-traps/RELEASE_NOTES/#version-031","text":"Interum release with Rust support for 2 new power monitoring events. Removal of image_uuid field from ImageLabelScore type used in ImageScoredEvent. MonitorPowerStartEvent and MonitorPowerStopEvent implemented in Rust (Python support in progress). The image_store_plugin deletes files of all types associated with an image when that image is deleted.","title":"Version 0.3.1"},{"location":"camera-traps/RELEASE_NOTES/#version-030","text":"Initial release of camera-traps images with the following features: Event Engine v0.2.0. Internal Rust production plugins image_recv_plugin, image_store_plugin and observer_plugin. The image_recv_plugin always writes the image to the configured images directory. The image_store_plugin determines if an images, based on its score, should be kept or deleted. If kept, its scores are written to a similarly named file with a .score suffix. Internal Rust test plugins image_gen_plugin and image_score_plugin. These plugins can be used when not running the cooresponding external plugins. External Python production plugins image_gen_plugin and image_score_plugin. The image_gen_plugin injects serveral built-in images into the application and image_score_plugin calls the Microsoft detector with its MegaDetector model to score the images for animal content.","title":"Version 0.3.0"},{"location":"camera-traps/custom_install/","text":"Camera Traps Custom Installer This directory contains the camera traps installer program to simplify the process of creating custom installations of the Camera Traps software based on a configuration file. Basic Usage To use the installer, execute the install.sh script within this directory, passing two positional arguments: Host directory: Absolute path on the host where the installer will look for input files and install Camera Traps. Input YAML file: path relative to Host directory where an input YAML file containing the configurations for the installation. This input is optional , the default value is install_config.yml Example invocation: $ ./install.sh /home/jstubbs/tmp/ct-installer my-input.yml In the example above, the installer will look for a file called my-input.yml with absolute path /home/jstubbs/tmp/ct-installer/my-input.yml with the configurations for the installation. Example contents of my-input.yml install_dir: test use_gpu_in_scoring: true The values in my-input.yml override defaults provided by the installer. In the example above, the following will happen: Camera Traps installer will install the main docker-compose.yml file and all necessary directories and additional files within the test directory of /home/jstubbs/tmp/ct-installer . The image scoring plugin will be configured to use GPUs on the host machine. All other default configurations will be used. See the following sections for details about the configurations available and defaults used. Required Configuration The Input YAML file must provide values for the following fields: install_dir : Relative path on the host (relative to Host directory) where the Camera Traps files will be installed. Example: test Important Optional Configurations ct_version : The release version (i.e., image tag) for the camera traps software container images. Example: 0.3.3 run_containers_as_user : Whether to run all containers as the user's (i.e., the installer's) UID and GID. (Default: true) Example: false use_gpu_in_scoring : Whether to use GPUs with the Image Scoring pluging. Note that the NVIDIA drivers must be installed, the NVIDIA container toolkit installed, and Docker must be configured to use it. See the main README for details on hardware requirements. (Default: false) Example: true image_store_save_threshold : The minimum confidence threshold for storing an image in full format. Example: 0.7 image_store_reduce_save_threshold : The minimum confidence threshold for storing an image in reduces/compressed format. Example: 0.4 device_id : The id of the device where the camera traps code will be installed. For example, this could be the id of a ChameleonCloud node or a TACC IoT device. By default, the string AAAAAAAAAAAAAAAAAAAA is assigned for cases where the device id will not be needed. Source Images One can control the source of images used for the simulation in various ways. By default, the installer users a pre-bundled set of example image. See below: use_bundled_example_images : Whether to use a set of bundled example images as the source of the input image. When true, the use_image_directory and use_image_url settings are ignored. (Default: true) use_image_directory : Whether to use a directory on the host as the source of the input images. If set to true, the source_image_dir variable must also be set. If set to false, a URL to a tar.gz file of images must be provided by setting use_image_url and source_image_url. (Default: false) source_image_dir : Host directory containing example images to be used for the simulation. Only used if use_image_directory is set to true; Example: example_images use_image_url : Whether to use a URL to a tar.gz file of images as the source of the input images. If set to true, the source_image_url variable must also be set. Example: true source_image_url : URL to a tar.gz file of images to use as the input images. Example: https://lilablobssc.blob.core.windows.net/snapshot-safari/KGA/KGA_S1.lila.zip use_custom_ground_truth_file_url : Whether to use a custom ground truth file with the dataset. This only needs to be set when using a non-standard dataset. Default is false. When setting to true, must also set the custom_ground_truth_file_url variable. custom_ground_truth_file_url : URL to a custom ground truth file to download. This variable is only used if use_custom_ground_truth_file_url is set to true. Specifying the Model use_custom_model_type : Use one of the pre-defined model types. When true, must also specify model_type . (Default: false). Example: true model_type : Specify the custom model type to use. Currently supported values are: 1 , 2 , 3 . This is a work in progress, more details coming. use_model_url : Use a URL to a compatible ML model .pt file. Will be downloaded at the start of the scoring plugin execution. If true, must also specify model_url. Example: false model_url ( currently not supported ): The URL to a model to download. Only used if use_model_url is true. Example: N/A Integrating with CKN deploy_ckn : Whether to deploy the CKN capture daemon; required for integration with CKN. Default is false. Example: true ckn_kafka_broker_address : The remote address of the CKN Kafka broker. Example: 129.114.35.150 ckn_kafka_broker_port : The remote port of the CKN Kafka broker. Example: 9092 experiment_id : An id to associate all measurements from this execution with. For example, a Tapis Job id. By default, this variable is set to none. user_id : The user id of the owner of the experiment. By default, this variable is set to none. All Configurations This is a complete list of all possible configurations. install_dir : Relative path on the host (relative to Host directory) where the Camera Traps files will be installed. ** Note ** all config paths are relative to this directory. Example: test ct_version : The release version (i.e., image tag) for the camera traps software container images Example: 0.3.3 host_config_dir : Path on the host where configuration directory resides. Example: ./config use_host_pid : Whether to start all plugin containers in the host PID namespace. Example: true run_containers_as_user : Whether to run all containers as the user's (i.e., the installer's) UID and GID Example: true image_generating_monitor_power : Whether to monitor the power usage of the image generating plugin. Default: true Example: false use_bundled_example_images : Whether to use a set of bundled example images as the source of the input image. When true, the use_image_directory and use_image_url settings are ignored. use_image_directory : Whether to use a directory on the host as the source of the input images. If set to true, the source_image_dir variable must also be set. If set to false, a URL to a tar.gz file of images must be provided by setting use_image_url and source_image_url. Example: true source_image_dir : Host directory containing example images to be used for the simulation. Only used if use_image_directory is set to true; Example: example_images use_image_url : Whether to use a URL to a tar.gz file of images as the source of the input images. If set to true, the source_image_url variable must also be set. Example: true source_image_url : URL to a tar.gz file of images to use as the input images. Example: https://lilablobssc.blob.core.windows.net/snapshot-safari/KGA/KGA_S1.lila.zip use_gpu_in_scoring : Whether to use GPUs with the Image Scoring pluging. Note that the NVIDIA drivers must be installed, the NVIDIA container toolkit installed, and Docker must be configured to use it. See the main README for details on hardware requirements. Example: true host_output_dir : Host directory where output directories will be written. (relative to installation directory) Example: output images_output_dir : Host directory within the host_output_dir where the output images will be written. (Relative to host_output_dir ) Example: images_output_dir image_store_save_threshold : The minimum confidence threshold for storing an image in full format. Example: 0.7 image_store_reduce_save_threshold : The minimum confidence threshold for storing an image in reduces/compressed format. Example: 0.4 oracle_plugin_output_dir : Host directory within the host_output_dir where the oracle_plugin's outputs will be written. (Relative to host_output_dir ) Example: oracle_output_dir image_generating_log_level : Log level for image generating plugin. Example: DEBUG, INFO image_scoring_plugin_image : The image to use for the image scoring plugin, not including the tag Example: tapis/image_scoring_plugin_py_3.8 image_scoring_log_level : Log level for image scoring plugin. Example: DEBUG, INFO image_scoring_monitor_power : Whether to monitor the power usage of the image scoring plugin. Default: true Example: false image_scoring_crop_images : Whether to crop images as part of the image scoring ML pipeline. (Default: false) Example: true image_scoring_generate_bounding_boxes : Whether to generate images with bounding boxes as part of the image scoring ML pipeline. Example: false use_custom_model_type : Use one of the pre-defined model types. When true, must also specify model_type . (Default: false). Example: true model_type : Specify the custom model type to use. Currently supported values are: 1 , 2 , 3 . This is a work in progress, more details coming. use_model_url : Use a URL to a compatible ML model .pt file. Will be downloaded at the start of the scoring plugin execution. If true, must also specify model_url. Example: false model_url ( currently not supported ): The URL to a model to download. Only used if use_model_url is true. Example: N/A run_power_monitor_privileged : Whether to run the power monitor plugin container in privileged mode. Note: this is required to read the Intel RAPL (Linux Powercap) interface and power monitor backends may not function without it. (Default: true) Example: false use_gpu_in_power_monitoring : Whether to use GPUs with the Power Monitroing pluging. At this time, there is no need to use GPUs in Power Monitoring, even if using them in Image Scoring, but this could change in a future release. Note that the NVIDIA drivers must be installed, the NVIDIA container toolkit installed, and Docker must be configured to use it. See the README for details on hardware requirements. (Default: false) Example: true power_monitor_log_level : Log level for power monitoring plugin. Example: DEBUG, INFO power_monitor_backend : The backend to use for measuring power. Example: powerjoular, scaphandre power_plugin_monitor_power : Whether to monitor the power of the power monitoring plugin itself. (Default: true) Example: false power_output_dir : Host directory within the host_output_dir where the output of the power plugin will be written. (Relative to host_output_dir ) Example: power_output_dir power_monitor_mount_docker_socket : Whether to mount the docker socket into the power monitor plugin container. Required when using the powerjoular backend. (Default: true). Example: false oracle_plugin_image : The image to use for the oracle plugin, not including the tag. (Default: tapis/oracle_plugin) Example: tapis/oracle_plugin","title":"Camera Traps Custom Installer"},{"location":"camera-traps/custom_install/#camera-traps-custom-installer","text":"This directory contains the camera traps installer program to simplify the process of creating custom installations of the Camera Traps software based on a configuration file.","title":"Camera Traps Custom Installer"},{"location":"camera-traps/custom_install/#basic-usage","text":"To use the installer, execute the install.sh script within this directory, passing two positional arguments: Host directory: Absolute path on the host where the installer will look for input files and install Camera Traps. Input YAML file: path relative to Host directory where an input YAML file containing the configurations for the installation. This input is optional , the default value is install_config.yml Example invocation: $ ./install.sh /home/jstubbs/tmp/ct-installer my-input.yml In the example above, the installer will look for a file called my-input.yml with absolute path /home/jstubbs/tmp/ct-installer/my-input.yml with the configurations for the installation. Example contents of my-input.yml install_dir: test use_gpu_in_scoring: true The values in my-input.yml override defaults provided by the installer. In the example above, the following will happen: Camera Traps installer will install the main docker-compose.yml file and all necessary directories and additional files within the test directory of /home/jstubbs/tmp/ct-installer . The image scoring plugin will be configured to use GPUs on the host machine. All other default configurations will be used. See the following sections for details about the configurations available and defaults used.","title":"Basic Usage"},{"location":"camera-traps/custom_install/#required-configuration","text":"The Input YAML file must provide values for the following fields: install_dir : Relative path on the host (relative to Host directory) where the Camera Traps files will be installed. Example: test","title":"Required Configuration"},{"location":"camera-traps/custom_install/#important-optional-configurations","text":"ct_version : The release version (i.e., image tag) for the camera traps software container images. Example: 0.3.3 run_containers_as_user : Whether to run all containers as the user's (i.e., the installer's) UID and GID. (Default: true) Example: false use_gpu_in_scoring : Whether to use GPUs with the Image Scoring pluging. Note that the NVIDIA drivers must be installed, the NVIDIA container toolkit installed, and Docker must be configured to use it. See the main README for details on hardware requirements. (Default: false) Example: true image_store_save_threshold : The minimum confidence threshold for storing an image in full format. Example: 0.7 image_store_reduce_save_threshold : The minimum confidence threshold for storing an image in reduces/compressed format. Example: 0.4 device_id : The id of the device where the camera traps code will be installed. For example, this could be the id of a ChameleonCloud node or a TACC IoT device. By default, the string AAAAAAAAAAAAAAAAAAAA is assigned for cases where the device id will not be needed. Source Images One can control the source of images used for the simulation in various ways. By default, the installer users a pre-bundled set of example image. See below: use_bundled_example_images : Whether to use a set of bundled example images as the source of the input image. When true, the use_image_directory and use_image_url settings are ignored. (Default: true) use_image_directory : Whether to use a directory on the host as the source of the input images. If set to true, the source_image_dir variable must also be set. If set to false, a URL to a tar.gz file of images must be provided by setting use_image_url and source_image_url. (Default: false) source_image_dir : Host directory containing example images to be used for the simulation. Only used if use_image_directory is set to true; Example: example_images use_image_url : Whether to use a URL to a tar.gz file of images as the source of the input images. If set to true, the source_image_url variable must also be set. Example: true source_image_url : URL to a tar.gz file of images to use as the input images. Example: https://lilablobssc.blob.core.windows.net/snapshot-safari/KGA/KGA_S1.lila.zip use_custom_ground_truth_file_url : Whether to use a custom ground truth file with the dataset. This only needs to be set when using a non-standard dataset. Default is false. When setting to true, must also set the custom_ground_truth_file_url variable. custom_ground_truth_file_url : URL to a custom ground truth file to download. This variable is only used if use_custom_ground_truth_file_url is set to true. Specifying the Model use_custom_model_type : Use one of the pre-defined model types. When true, must also specify model_type . (Default: false). Example: true model_type : Specify the custom model type to use. Currently supported values are: 1 , 2 , 3 . This is a work in progress, more details coming. use_model_url : Use a URL to a compatible ML model .pt file. Will be downloaded at the start of the scoring plugin execution. If true, must also specify model_url. Example: false model_url ( currently not supported ): The URL to a model to download. Only used if use_model_url is true. Example: N/A Integrating with CKN deploy_ckn : Whether to deploy the CKN capture daemon; required for integration with CKN. Default is false. Example: true ckn_kafka_broker_address : The remote address of the CKN Kafka broker. Example: 129.114.35.150 ckn_kafka_broker_port : The remote port of the CKN Kafka broker. Example: 9092 experiment_id : An id to associate all measurements from this execution with. For example, a Tapis Job id. By default, this variable is set to none. user_id : The user id of the owner of the experiment. By default, this variable is set to none.","title":"Important Optional Configurations"},{"location":"camera-traps/custom_install/#all-configurations","text":"This is a complete list of all possible configurations. install_dir : Relative path on the host (relative to Host directory) where the Camera Traps files will be installed. ** Note ** all config paths are relative to this directory. Example: test ct_version : The release version (i.e., image tag) for the camera traps software container images Example: 0.3.3 host_config_dir : Path on the host where configuration directory resides. Example: ./config use_host_pid : Whether to start all plugin containers in the host PID namespace. Example: true run_containers_as_user : Whether to run all containers as the user's (i.e., the installer's) UID and GID Example: true image_generating_monitor_power : Whether to monitor the power usage of the image generating plugin. Default: true Example: false use_bundled_example_images : Whether to use a set of bundled example images as the source of the input image. When true, the use_image_directory and use_image_url settings are ignored. use_image_directory : Whether to use a directory on the host as the source of the input images. If set to true, the source_image_dir variable must also be set. If set to false, a URL to a tar.gz file of images must be provided by setting use_image_url and source_image_url. Example: true source_image_dir : Host directory containing example images to be used for the simulation. Only used if use_image_directory is set to true; Example: example_images use_image_url : Whether to use a URL to a tar.gz file of images as the source of the input images. If set to true, the source_image_url variable must also be set. Example: true source_image_url : URL to a tar.gz file of images to use as the input images. Example: https://lilablobssc.blob.core.windows.net/snapshot-safari/KGA/KGA_S1.lila.zip use_gpu_in_scoring : Whether to use GPUs with the Image Scoring pluging. Note that the NVIDIA drivers must be installed, the NVIDIA container toolkit installed, and Docker must be configured to use it. See the main README for details on hardware requirements. Example: true host_output_dir : Host directory where output directories will be written. (relative to installation directory) Example: output images_output_dir : Host directory within the host_output_dir where the output images will be written. (Relative to host_output_dir ) Example: images_output_dir image_store_save_threshold : The minimum confidence threshold for storing an image in full format. Example: 0.7 image_store_reduce_save_threshold : The minimum confidence threshold for storing an image in reduces/compressed format. Example: 0.4 oracle_plugin_output_dir : Host directory within the host_output_dir where the oracle_plugin's outputs will be written. (Relative to host_output_dir ) Example: oracle_output_dir image_generating_log_level : Log level for image generating plugin. Example: DEBUG, INFO image_scoring_plugin_image : The image to use for the image scoring plugin, not including the tag Example: tapis/image_scoring_plugin_py_3.8 image_scoring_log_level : Log level for image scoring plugin. Example: DEBUG, INFO image_scoring_monitor_power : Whether to monitor the power usage of the image scoring plugin. Default: true Example: false image_scoring_crop_images : Whether to crop images as part of the image scoring ML pipeline. (Default: false) Example: true image_scoring_generate_bounding_boxes : Whether to generate images with bounding boxes as part of the image scoring ML pipeline. Example: false use_custom_model_type : Use one of the pre-defined model types. When true, must also specify model_type . (Default: false). Example: true model_type : Specify the custom model type to use. Currently supported values are: 1 , 2 , 3 . This is a work in progress, more details coming. use_model_url : Use a URL to a compatible ML model .pt file. Will be downloaded at the start of the scoring plugin execution. If true, must also specify model_url. Example: false model_url ( currently not supported ): The URL to a model to download. Only used if use_model_url is true. Example: N/A run_power_monitor_privileged : Whether to run the power monitor plugin container in privileged mode. Note: this is required to read the Intel RAPL (Linux Powercap) interface and power monitor backends may not function without it. (Default: true) Example: false use_gpu_in_power_monitoring : Whether to use GPUs with the Power Monitroing pluging. At this time, there is no need to use GPUs in Power Monitoring, even if using them in Image Scoring, but this could change in a future release. Note that the NVIDIA drivers must be installed, the NVIDIA container toolkit installed, and Docker must be configured to use it. See the README for details on hardware requirements. (Default: false) Example: true power_monitor_log_level : Log level for power monitoring plugin. Example: DEBUG, INFO power_monitor_backend : The backend to use for measuring power. Example: powerjoular, scaphandre power_plugin_monitor_power : Whether to monitor the power of the power monitoring plugin itself. (Default: true) Example: false power_output_dir : Host directory within the host_output_dir where the output of the power plugin will be written. (Relative to host_output_dir ) Example: power_output_dir power_monitor_mount_docker_socket : Whether to mount the docker socket into the power monitor plugin container. Required when using the powerjoular backend. (Default: true). Example: false oracle_plugin_image : The image to use for the oracle plugin, not including the tag. (Default: tapis/oracle_plugin) Example: tapis/oracle_plugin","title":"All Configurations"},{"location":"camera-traps/external_plugins/image_generating_plugin/","text":"Image Generating Plugin A Python plugin for the Camera Traps project that simulated an image taking protocol. Building with Nix Build the flake (requires Nix and to enable flakes, still experimental). flake build","title":"Image Generating Plugin"},{"location":"camera-traps/external_plugins/image_generating_plugin/#image-generating-plugin","text":"A Python plugin for the Camera Traps project that simulated an image taking protocol.","title":"Image Generating Plugin"},{"location":"camera-traps/external_plugins/image_generating_plugin/#building-with-nix","text":"Build the flake (requires Nix and to enable flakes, still experimental). flake build","title":"Building with Nix"},{"location":"camera-traps/external_plugins/image_scoring_plugin/","text":"Image Scoring Plugin The image scoring plugin uses computer vision algorithms to \"score\" images based on the likelihood of the image containing animals of interest. The current implementation makes use of the MicroSoft MegaDetector package, which in turn is based on Yolov5. Docker Image Two docker images are currently available -- one based on Python 3.8 and one based on Python 3.10. These images are designed to run on x86/64 bit Linux and, optionally, Nvidia GPU. We are currently working on additional container images for ARM architectures, including edge devices such as the Jetson series. Nix We are also exploring packaging the image scoring plugin code using Nix. The flake in the repo can be used to build a development environment: nix develop This builds the flake and drops you into a Nix environment with Python 3.8.16, the image scoring plugin source code as well as all dependencies installed. From there, the load tests can be run with: python `realpath load_test_image_scoring.py` Note that realpath is required at this time, as the image scoring plugin code currently makes certain assumptions about the location of python relative to the model file.","title":"Image Scoring Plugin"},{"location":"camera-traps/external_plugins/image_scoring_plugin/#image-scoring-plugin","text":"The image scoring plugin uses computer vision algorithms to \"score\" images based on the likelihood of the image containing animals of interest. The current implementation makes use of the MicroSoft MegaDetector package, which in turn is based on Yolov5.","title":"Image Scoring Plugin"},{"location":"camera-traps/external_plugins/image_scoring_plugin/#docker-image","text":"Two docker images are currently available -- one based on Python 3.8 and one based on Python 3.10. These images are designed to run on x86/64 bit Linux and, optionally, Nvidia GPU. We are currently working on additional container images for ARM architectures, including edge devices such as the Jetson series.","title":"Docker Image"},{"location":"camera-traps/external_plugins/image_scoring_plugin/#nix","text":"We are also exploring packaging the image scoring plugin code using Nix. The flake in the repo can be used to build a development environment: nix develop This builds the flake and drops you into a Nix environment with Python 3.8.16, the image scoring plugin source code as well as all dependencies installed. From there, the load tests can be run with: python `realpath load_test_image_scoring.py` Note that realpath is required at this time, as the image scoring plugin code currently makes certain assumptions about the location of python relative to the model file.","title":"Nix"},{"location":"camera-traps/external_plugins/power_measuring_plugin/","text":"Power Measure Plugin Power consumption in inferencing AI models is a critical consideration, particularly for edge devices. High power consumption can lead to reduced battery life, increased costs, and more frequent charging or battery replacement intervals. For AI models to be effectively integrated and operational in real-world scenarios, they need to be efficient, with minimal power demands. Efficient models enable faster computation, longer device uptime, and wider adoption in applications where constant power sources are unavailable. As AI continues to penetrate various sectors, optimizing power consumption during inferencing ensures the sustainable growth and usability of AI-driven solutions in our everyday lives. Power measuring tools play a pivotal role in monitoring the power consumption of AI models during inference, especially in edge devices. As these models process data and make decisions in real time, understanding their power demands becomes crucial. By accurately gauging power usage, developers and system managers can dynamically adjust workloads, balance system resources, and optimize model parameters for energy efficiency. This real-time adjustment ensures that devices remain responsive, maximize battery life, and avoid potential overheating or other power-related issues. Furthermore, consistent power monitoring provides invaluable data for refining algorithms and hardware architectures. In essence, power measuring not only guarantees optimal performance during AI inference but also informs future advancements in AI hardware and software design, ensuring sustainable and efficient AI integration in various applications. Designs When inference AI models, we consider two important pieces of hardware that can consume most of the power on the device, i.e. CPU and GPU. CPU participates in data loading, pre-processing, post-processing, etc., while GPU will handle all the computation in AI models. Therefore, this plugin provides fine-grained monitoring of them. Hardware Requirements Backend API Sample Rate (per second) Notes CPU Intel RAPL interface Scaphandre 2 Process-level monitoring GPU NVIDIA NVML 2 (configurable) On-chip power monitoring The plugin spawns two threads at launching. - Thread 0 will spin on the socket to check if new power-measuring events are coming, if so, it adds them to the event queue. - Thread 1 will spin on the event queue, and read out events with the necessary information, e.g. monitoring type, duration, PIDs, start_time, etc. The plugin uses the system-level process to directly run the backend APIs and capture the output. By default, CPU-level monitoring can be fine-grained at the individual processes, while GPU only logs the overall power on chip. Usage In other plugins where you want to measure power, from ctevents.ctevents import send_power_measure_fb_event import os pids = [os.getpid(), ] # Could be any pids you want to measure monitor_type = [0, ] # 0 for CPU & GPU, 1 for CPU, 2 for GPU (TODO: 3 for DRAM) monitor_duration = 10 # Seconds send_power_measure_fb_event(socket, pids, monitor_type, monitor_duration) Default log files will be saved under ~/logs , and CPU and GPU logs will be separated. We provide a test case in our plugin, set the environment TRAPS_TEST_POWER_FUNCTION=1 , and the plugin will log its own power consumption for 10 seconds. Example of output ~/logs/CPU.json : {\"2023-08-25 05:50:59\": [8.369286, \"337263\", \"python\"], \"2023-08-25 05:51:02\": [8.153751, \"337263\", \"python\"], \"2023-08-25 05:51:04\": [7.726121, \"337263\", \"python\"], \"2023-08-25 05:51:06\": [8.300131, \"337263\", \"python\"]} The dict keys are the time stamps, for every process, the first element is the power in Watts, the second is the PID, and the third is the process name. ~/logs/GPU.json : {\"2023-08-25 05:50:57\": 48.57, \"2023-08-25 05:51:00\": 47.49, \"2023-08-25 05:51:02\": 53.4, \"2023-08-25 05:51:04\": 47.5, \"2023-08-25 05:51:06\": 49.96} The dict keys are the time stamps, since NVIDIA does not support process-level monitoring, we only record the overall on-chip power(in Watts).","title":"Power Measure Plugin"},{"location":"camera-traps/external_plugins/power_measuring_plugin/#power-measure-plugin","text":"Power consumption in inferencing AI models is a critical consideration, particularly for edge devices. High power consumption can lead to reduced battery life, increased costs, and more frequent charging or battery replacement intervals. For AI models to be effectively integrated and operational in real-world scenarios, they need to be efficient, with minimal power demands. Efficient models enable faster computation, longer device uptime, and wider adoption in applications where constant power sources are unavailable. As AI continues to penetrate various sectors, optimizing power consumption during inferencing ensures the sustainable growth and usability of AI-driven solutions in our everyday lives. Power measuring tools play a pivotal role in monitoring the power consumption of AI models during inference, especially in edge devices. As these models process data and make decisions in real time, understanding their power demands becomes crucial. By accurately gauging power usage, developers and system managers can dynamically adjust workloads, balance system resources, and optimize model parameters for energy efficiency. This real-time adjustment ensures that devices remain responsive, maximize battery life, and avoid potential overheating or other power-related issues. Furthermore, consistent power monitoring provides invaluable data for refining algorithms and hardware architectures. In essence, power measuring not only guarantees optimal performance during AI inference but also informs future advancements in AI hardware and software design, ensuring sustainable and efficient AI integration in various applications.","title":"Power Measure Plugin"},{"location":"camera-traps/external_plugins/power_measuring_plugin/#designs","text":"When inference AI models, we consider two important pieces of hardware that can consume most of the power on the device, i.e. CPU and GPU. CPU participates in data loading, pre-processing, post-processing, etc., while GPU will handle all the computation in AI models. Therefore, this plugin provides fine-grained monitoring of them. Hardware Requirements Backend API Sample Rate (per second) Notes CPU Intel RAPL interface Scaphandre 2 Process-level monitoring GPU NVIDIA NVML 2 (configurable) On-chip power monitoring The plugin spawns two threads at launching. - Thread 0 will spin on the socket to check if new power-measuring events are coming, if so, it adds them to the event queue. - Thread 1 will spin on the event queue, and read out events with the necessary information, e.g. monitoring type, duration, PIDs, start_time, etc. The plugin uses the system-level process to directly run the backend APIs and capture the output. By default, CPU-level monitoring can be fine-grained at the individual processes, while GPU only logs the overall power on chip.","title":"Designs"},{"location":"camera-traps/external_plugins/power_measuring_plugin/#usage","text":"In other plugins where you want to measure power, from ctevents.ctevents import send_power_measure_fb_event import os pids = [os.getpid(), ] # Could be any pids you want to measure monitor_type = [0, ] # 0 for CPU & GPU, 1 for CPU, 2 for GPU (TODO: 3 for DRAM) monitor_duration = 10 # Seconds send_power_measure_fb_event(socket, pids, monitor_type, monitor_duration) Default log files will be saved under ~/logs , and CPU and GPU logs will be separated. We provide a test case in our plugin, set the environment TRAPS_TEST_POWER_FUNCTION=1 , and the plugin will log its own power consumption for 10 seconds.","title":"Usage"},{"location":"camera-traps/external_plugins/power_measuring_plugin/#example-of-output","text":"~/logs/CPU.json : {\"2023-08-25 05:50:59\": [8.369286, \"337263\", \"python\"], \"2023-08-25 05:51:02\": [8.153751, \"337263\", \"python\"], \"2023-08-25 05:51:04\": [7.726121, \"337263\", \"python\"], \"2023-08-25 05:51:06\": [8.300131, \"337263\", \"python\"]} The dict keys are the time stamps, for every process, the first element is the power in Watts, the second is the PID, and the third is the process name. ~/logs/GPU.json : {\"2023-08-25 05:50:57\": 48.57, \"2023-08-25 05:51:00\": 47.49, \"2023-08-25 05:51:02\": 53.4, \"2023-08-25 05:51:04\": 47.5, \"2023-08-25 05:51:06\": 49.96} The dict keys are the time stamps, since NVIDIA does not support process-level monitoring, we only record the overall on-chip power(in Watts).","title":"Example of output"},{"location":"camera-traps/releases/","text":"Running Released Versions of Camera-Traps Running the camera-traps application as described here requires docker to be installed. Later versions of docker include docker-compose, earlier versions may require a separate installation of docker-compose. Each of the subdirectories in this directory corresponds to a released or under development version of the camera-traps application. Usually, there will be at most only one release under development, the one with the highest release number. Each release has its own tagged images in docker hub. When invoked, a release's docker-compose file will start the application and its internal plugins in one container. It will also start all configured external plugins in their own containers. By default, the external image_generating_plugin reads a small set of sample images from a configured directory and injects them into the application. Successful processing of these sample images indicates that the application is functional. The image_generating_plugin container terminates after reading the image directory. One way to inject your own images into the application is to configure the image_generating_plugin to read from a directory containing images of your choosing. See the config/image_gen_config.json for details. Release Files Each release can have its own customized configuration files. A docker-compose.yml file will always reside in the release's top-level directory; other configuration files will be in the config subdirectory. Here are files you may encounter: docker-compose.yml - the file that configures all containers. It is used to bring up and tear down the application and its external plugins. config/traps.toml - the camera-traps application configuration file that specfies the internal and external plugins that will run. config/image_gen_config.json - configuration setting for the image_generating_plugin , including the designated input images directory. config/\\ - each plugin can have it's own toml or json configuration file. The releases/common directory contains data and configuration files available to all releases. In addition, log configuration is managed using the resources/log4rs.yml file. Example: Running the Latest Release cd latest docker-compose up There should be no errors as the application processes each sample image. The image_generating_plugin container will exit when all images are read from the configured input directory. The output images and scores will be in the release's images_output_dir directory. To shutdown the application, issue this command from another terminal: docker-compose down Integration Tests Some releases will include in integration test subdirectory. These tests can be useful to developers for debugging and require a Rust development environment. The runtest.sh script invokes the tests. A Note on Compilation Even though one can run current and past releases from the releases directory, code compilation and image generation is tied to the code version currently checked out from the source code repository . See the top-level README file for build instructions and our development process.","title":"Running Released Versions of Camera-Traps"},{"location":"camera-traps/releases/#running-released-versions-of-camera-traps","text":"Running the camera-traps application as described here requires docker to be installed. Later versions of docker include docker-compose, earlier versions may require a separate installation of docker-compose. Each of the subdirectories in this directory corresponds to a released or under development version of the camera-traps application. Usually, there will be at most only one release under development, the one with the highest release number. Each release has its own tagged images in docker hub. When invoked, a release's docker-compose file will start the application and its internal plugins in one container. It will also start all configured external plugins in their own containers. By default, the external image_generating_plugin reads a small set of sample images from a configured directory and injects them into the application. Successful processing of these sample images indicates that the application is functional. The image_generating_plugin container terminates after reading the image directory. One way to inject your own images into the application is to configure the image_generating_plugin to read from a directory containing images of your choosing. See the config/image_gen_config.json for details.","title":"Running Released Versions of Camera-Traps"},{"location":"camera-traps/releases/#release-files","text":"Each release can have its own customized configuration files. A docker-compose.yml file will always reside in the release's top-level directory; other configuration files will be in the config subdirectory. Here are files you may encounter: docker-compose.yml - the file that configures all containers. It is used to bring up and tear down the application and its external plugins. config/traps.toml - the camera-traps application configuration file that specfies the internal and external plugins that will run. config/image_gen_config.json - configuration setting for the image_generating_plugin , including the designated input images directory. config/\\ - each plugin can have it's own toml or json configuration file. The releases/common directory contains data and configuration files available to all releases. In addition, log configuration is managed using the resources/log4rs.yml file.","title":"Release Files"},{"location":"camera-traps/releases/#example-running-the-latest-release","text":"cd latest docker-compose up There should be no errors as the application processes each sample image. The image_generating_plugin container will exit when all images are read from the configured input directory. The output images and scores will be in the release's images_output_dir directory. To shutdown the application, issue this command from another terminal: docker-compose down","title":"Example: Running the Latest Release"},{"location":"camera-traps/releases/#integration-tests","text":"Some releases will include in integration test subdirectory. These tests can be useful to developers for debugging and require a Rust development environment. The runtest.sh script invokes the tests.","title":"Integration Tests"},{"location":"camera-traps/releases/#a-note-on-compilation","text":"Even though one can run current and past releases from the releases directory, code compilation and image generation is tied to the code version currently checked out from the source code repository . See the top-level README file for build instructions and our development process.","title":"A Note on Compilation"},{"location":"camera-traps/releases/0.3.1/integration-test/","text":"Building and Running the Integration Test The integration test described here simply injects images into an instance of the camera-traps application configured for testing with internal plugins (only one process is started). The application runs in a docker container whereas the integration test is invoked by cargo. To allow the test to communicate with the application, the latter runs on the host network rather than a private docker network. Though unlikely, failures are possible if the configured ports are already in use. The integration test executable must exist in order for cargo to invoke it. This typically requires a Rust development environment to run and, if necessary, build the test program. Commands The integration test requires the camera-traps application to be running before it is manually invoked. All commands should be issued from the directory in which this file resides. To run an instance of camera-traps configured for the integration test, issue this command in a command-line terminal: docker-compose up To run the integration test, issue: ./runtest.sh To shutdown the camera-traps application, issue: docker-compose down Output When started in a terminal, the application will log to standard output an indication that these four internal plugins have been loaded: image_recv_plugin image_score_plugin image_store_plugin observer_plugin The application will also indicate that an external plugin port was opened: ext_image_gen_test_plugin Other informational records are also written when the application starts. When runtest.sh executes, it will read the traps-integration.toml file to configure itself. That file includes the following information. The number of total number of times an image is injected into camera-traps ( iterations ). The directory where at least one image file can be found ( image_input_dir ). The external plugin configuration ( external_plugin_config ). Test execution will cause four events to be logged by the observer plugin for each image received. These events are: NewImageEvent ImageReceivedEvent ImageScoredEvent ImageStoredEvent The level of logging detail can be controlled by modifying the log4rs.yml file in this directory.","title":"Building and Running the Integration Test"},{"location":"camera-traps/releases/0.3.1/integration-test/#building-and-running-the-integration-test","text":"The integration test described here simply injects images into an instance of the camera-traps application configured for testing with internal plugins (only one process is started). The application runs in a docker container whereas the integration test is invoked by cargo. To allow the test to communicate with the application, the latter runs on the host network rather than a private docker network. Though unlikely, failures are possible if the configured ports are already in use. The integration test executable must exist in order for cargo to invoke it. This typically requires a Rust development environment to run and, if necessary, build the test program.","title":"Building and Running the Integration Test"},{"location":"camera-traps/releases/0.3.1/integration-test/#commands","text":"The integration test requires the camera-traps application to be running before it is manually invoked. All commands should be issued from the directory in which this file resides. To run an instance of camera-traps configured for the integration test, issue this command in a command-line terminal: docker-compose up To run the integration test, issue: ./runtest.sh To shutdown the camera-traps application, issue: docker-compose down","title":"Commands"},{"location":"camera-traps/releases/0.3.1/integration-test/#output","text":"When started in a terminal, the application will log to standard output an indication that these four internal plugins have been loaded: image_recv_plugin image_score_plugin image_store_plugin observer_plugin The application will also indicate that an external plugin port was opened: ext_image_gen_test_plugin Other informational records are also written when the application starts. When runtest.sh executes, it will read the traps-integration.toml file to configure itself. That file includes the following information. The number of total number of times an image is injected into camera-traps ( iterations ). The directory where at least one image file can be found ( image_input_dir ). The external plugin configuration ( external_plugin_config ). Test execution will cause four events to be logged by the observer plugin for each image received. These events are: NewImageEvent ImageReceivedEvent ImageScoredEvent ImageStoredEvent The level of logging detail can be controlled by modifying the log4rs.yml file in this directory.","title":"Output"},{"location":"camera-traps/src/python/","text":"ctevents A python library for working with the Camera Traps events messages. This library provided high-level convenience functions for easily sending and receiving Camera Traps events. There are two Docker images associated with this package, tapis/camera_traps_py and tapis/camera_traps_py_3.8; each image provides a tag for each release, for example: tapis/camera_traps_py:0.3.3 and tapis/camera_traps_py_3.8:0.3.3 for the 0.3.3 release. The images bundle a test file that can be executed directly in the image; for example: # start a container docker run -it --rm --entrypoint=bash tapis/camera_traps_py:0.3.3 # run the tests python test_ctevents.py The test suite was not written with any test framework so no reporting is provided. Since all that was used was simple assert statements, if the program executes completely (with no exception output), then the tests have all \"passed\".","title":"ctevents"},{"location":"camera-traps/src/python/#ctevents","text":"A python library for working with the Camera Traps events messages. This library provided high-level convenience functions for easily sending and receiving Camera Traps events. There are two Docker images associated with this package, tapis/camera_traps_py and tapis/camera_traps_py_3.8; each image provides a tag for each release, for example: tapis/camera_traps_py:0.3.3 and tapis/camera_traps_py_3.8:0.3.3 for the 0.3.3 release. The images bundle a test file that can be executed directly in the image; for example: # start a container docker run -it --rm --entrypoint=bash tapis/camera_traps_py:0.3.3 # run the tests python test_ctevents.py The test suite was not written with any test framework so no reporting is provided. Since all that was used was simple assert statements, if the program executes completely (with no exception output), then the tests have all \"passed\".","title":"ctevents"},{"location":"harp/","text":"HARP - HPC Application Runtime Predictor Overview Researchers use high-performance computing (HPC) cyberinfrastructures (CI) like the Ohio Supercomputer (OSC) or Texas Advanced Computing Center (TACC) to execute computationally intensive diverse scientific workflows. Some workflows are heavy on IO, like genome sequencing (cleaning and assembly), while others, like training DNNs, could be compute (and memory) intensive. Each workflow has a unique resource requirement, and it is essential to profile and understand these needs to allocate shared resources for optimal utilization of the cyberinfrastructure. These resources are expensive, and several jobs compete to get these allocations, sometimes with reasonable wait times (while requesting enormous resources for a long time). Estimating the expected resources for optimally utilizing the compute and memory is challenging, especially considering the need for sufficient history to enable these predictions tailored for unique workflows and execution environments. We explored and established a framework (as shown in Figure 1) that pipelines the solutions to address these challenges. The Framework is configured to generate a history of executions and train suitable regression models to estimate the approximate execution time for a targeted application. Figure 1: The Proposed Framework: training data generation, building regression models, selecting the best model based on custom criteria Components and Characteristics of the Framework (from Figure 1): Generating and Preparing Training Data: This module automatically and systematically generates comprehensive, diverse \"scaled-down(SD)\" and limited, selective \"full-scale(FS)\" runs with minimal human intervention. We use Cheetah (https://github.com/CODARcode/cheetah) to execute the target application with the pre-defined data generation configurations (SD and FS) to generate the history-of-runs training data. Building Regression Models: This module standardizes and prepares the data, trains the selected off-the-shelf regression models with the appropriate hyper-parameters, and stores them for inference. In this phase, the data generated in the first phase is processed to train regression models. Redundant features are eliminated, outliers are removed, and features are transformed to reduce the dimensionality before training the regression models. Selecting Appropriate Prediction Model: This module selects the most appropriate regression model from a pool of trained models from phase 2 with respect to a given policy and target application Note: The Framework is built on TensorFlow Framework. Figure 2: Shows the target-application execution endpoint and the harp application folder structure. WAYS to configure HARP to setup applications for profiling: Configure HARP with TAPIS to execute HARP and applications to profile as container images. Install HARP as a loadable module on OSC or localbox. WAY 1. Configure HARP with TAPIS to configure HARP and applications to profile as container images. NOTE: IMPORTANT INFORMATION - PLEASE READ 1. TAPIS executes only containerized applications, so HARP (HARP framework) and the Application should be containerized. 2. Profiling an application HARP Framework with TAPIS is tested on both TACC and OSC systems. 3. Colab Notebooks server as examples for running HARP profiling for a sample Euler application using TAPIS. Make a COPY of the colab or download the Notebooks to execute them. 4. HARP Framework and example containers could be executed on local box user docker or apptaner servers without TAPIS integration. Follow through the Colab notebooks for profiling the HARP Container application using TAPIS: 1. For TACC stampede2 example use: https://drive.google.com/file/d/1JyAHUxxZ3pKMXGs28UXMQJn5QZmti1yS/view?usp=sharing 2. For OSC pitzer example use:https://drive.google.com/file/d/1w8qCTWiOjvn8CCx6FqvZzG8ZJBRKy4M3/view?usp=sharing Alternatively, download the example notebook from the \"Notebooks\" folder Steps: 1.Create a new HARP Image [or] using the pre-made HARP Image 2.Create an image for the application to be profiled using HARP Framework (Image) 3.Refer to Section \" Using HARP to profile an application and predict the execution time \" for steps to execute the container in localbox or a CI (like OSC or TACC nodes) with or without TAPIS. Create a HARP Image using Docker Environment a. Use the 'Dockerfile_HARP_local' file to create an Image for executing the Framework on a local box using 'docker build'. docker build -f DockerFiles/Dockerfile_HARP_local -t harp-framework-local:2.0.0 . b. Use the Dockerfile_HARP_CI file to create an Image for executing the Framework on CI (like TACC or OSC) using 'docker build'. docker build -f DockerFiles/Dockerfile_HARP_CI -t harp-framework-ci:2.0.0 . c. Push the Image to the docker hub or upload it to any web-accessible location. Push the image to a web-accessible location like dockerhub using the following command: docker push <DockerHub>/harp-framework-[local|ci]:2.0.0 If you already have access to the pre-compiled images, use these copies of the HARP image from our repository a. Use the harp-framework-local:2.0.0 file for executing the Framework on a local box. b. Use the harp-framework-ci:2.0.0 file for executing the Framework on TACC or OSC Systems. Note : Execute the 'docker build' command from the main folder 'harp'. Create an image for the application to be profiled using HARP Framework (Image) Using the Image created in \"Step 1\" or the respective Image from the ICICLE repository, create an Image for the application to be profiled using the HARP Framework. We refer to the application to be profiled as a 'target application'. Steps for creating a DockeFile for the target application and building an image using the 'Dockerfile_App_Template' template. a. Edit the 'ProfileApplication.sh' entry point file to execute the HARP with the pipeline configurations JSON. Replace \"pipeline_config.json\" with your desired pipeline configuration file in the application work folder. harp pipeline_config.json b. Create a DockerFile for the target application from the following template FROM <web-accessible-path-to-image-hub>/harp-framework-[local|ci]:2.0.0 # 1. Add application required installations. # none for Euler example # 2. Target application setup # Set the APP_PATH to the location with your application work folder (target application to be profiled) and APP_NAME to the work folder name ENV APP_PATH=\"<path-to-target-application>/<target-application-work-folder>\" ENV APP_NAME=\"<target-application-work-folder>\" # Add the target application work folder to the Image ADD $APP_PATH /app/$APP_PATH # 3. Copy the execution endpoint file and set it COPY DockerFiles/ProfileApplication.sh /app/ProfileApplication.sh ENTRYPOINT [\"sh\", \"/app/ProfileApplication.sh\"] c. Build the application Image (these commands show building an image for an example application 'Euler Number') a. Use the 'Dockerfile_App_EulerNumber' file to create an Image for executing the Framework on a local box using 'docker build'. docker build -f DockerFiles/Dockerfile_App_EulerNumber -t harp-app-eulernumber-[local|ci]:2.0.0 . b. Push the Image to the docker hub or upload it to any web-accessible location. Push the image to a web-accessible location like dockerhub using the following command: docker push <DockerHub>/harp-app-eulernumber-[local|ci]:2.0.0 Use an existing copy of the Euler Number application image from our repository. Refer to Section \" Using HARP to profile an application and predict the execution time \" for steps to execute the container in localbox or a CI (like OSC or TACC nodes) with or without TAPIS. WAY 2. Installation-based HARP setup Dependency: Linux, Python 3.9+, git, pip, mpich, psutil, jq(command line JSON parser https://stedolan.github.io/jq/) On supercomputers (OSC), it should be installed at a location accessible from the parallel file system Follow these steps to set up HARP as a loadable software module on the Ohio Supercomputer (OSC): bash git clone https://github.com/ICICLE-ai/harp.git cd harp chmod 755 install-osc-harp.sh ./install-osc-harp.sh If the installation fails, please re-run the script 'install-osc-harp.sh' after deleting the environment 'harp-env' and running the cleanup.sh in the install directory. bash conda remove --name harp-env --all ./cleanup.sh This setup installs miniconda, CODAR Cheetah (https://github.com/CODARcode/cheetah), TensorFlow, psutil, pandas, and scikit-learn and configures the Harp framework. Please follow the installation prompts to go ahead with the setup. This installation takes 30-40 mins to finish the setup on Owens login node. Loading the HARP module on OSC bash module use $HOME/osc_apps/lmodfiles module load harp export CONDA_HOME=<path_to_miniconda>/miniconda3 source $CONDA_HOME/bin/activate source activate harp_env NOTE Things to consider while installing the Framework on OSC 1. [OSC Installation] The installer creates a conda environment, \"harp_env\" on OSC and uses this environment to execute the Framework. The environment name is used in a couple of Cheetash configurations and hence is mandated to use the same name, \"harp_env,\" while installing the application. Please delete the environment if it already exists with this name before installing the Framework. 2. Upon successful installation, the install script will return the below response: (OSC Install Script) Generating Module File Step: /users/PAS0536/swathivm/osc_apps/lmodfiles/harp/1.0.lua (OSC Install Script) Generating Module File Step Finished Finished at Thu Mar 16 11:44:13 EDT 2023 Execution time: 1965 seconds Follow these steps to set the HARP framework on a standalone Linux system: Use these commands to install the dependencies using pip bash pip install psutil pip install tensorflow pip install pandas pip install scikit-learn Download the source code into the and set it to HARP_HOME bash git clone https://github.com/ICICLE-ai/harp.git export HARP_HOME=<path-to-download-folder>/harp Install Cheetah bash cd $HARP_HOME/cheetah pip install --editable . Ensure the scripts have ' execute ' privileges bash cd $HARP_HOME/pipeline/bin/local chmod 755 harp cd $HARP_HOME/cheetah/bin chmod 755 * Set the HARP pipeline and Cheetah binaries in the PATH bash export PATH=$HARP_HOME/pipeline/bin/local:$HARP_HOME/cheetah/bin:$PATH The HARP pipeline is ready to be used once the HARP_HOME and binaries are set in PATH. NOTE HARP has been tested on Ownes and Pitzer (OSC) and a standalone Linux system. Things to consider while installing the dependencies on standalone Linux systems: 1. if you do not have root or admin privileges, please consult your package manager on installing mpich and operator dependencies. Using HARP to profile an application and predict the execution time Using the HARP (version 2.0.0) to profile an application (e.g., Euler Number) on local box and CIs (OSC and TACC) using TAPIS a. To profile the application, execute the application image built from the HARP parent image using the following commands on the localbox: [optional] docker pull ghcr.io/icicle-ai/harp-app-eulernumber-local:2.0.0 docker run --mount source=HARP_Store,target=/scratch ghcr.io/icicle-ai/harp-app-eulernumber-local:2.0.0 b. To profile the application using HARP on OSC or TACC i. Without TAPIS: Login into a compute node on OSC or TACC and run the following: for OSC Nodes module load singularity [optional] singularity pull docker://ghcr.io/icicle-ai/harp-app-eulernumber-ci:2.0.0 singularity run docker://ghcr.io/icicle-ai/harp-app-eulernumber-ci:2.0.0 osc /fs/scratch/PAS2271/swathivm/ For TACC Nodes (Stampede2) module load tacc-apptainer [optional] singularity pull docker://ghcr.io/icicle-ai/harp-app-eulernumber-ci:2.0.0 singularity run docker://ghcr.io/icicle-ai/harp-app-eulernumber-ci:2.0.0 tacc none ii. With TAPIS: Follow the instructions in the ' Executing_HARP_using_TAPIS.ipynb ' notebook in the 'Notebooks' folder to register OSC and TACC systems on TAPIS and profile the application 'harp-app-eulernumber-ci:2.0.0' using HARP Using the HARP (version 1.0.0) module, load the OSC of the HARP binary on the local box Navigate to the target application folder and copy all the files from /Post_Execution_Scripts/basic into the current folder. Edit path in post-script.sh to point to the target application directory Execute the Framework as per the configurations in file 'train_config.json' as follows: bash cd <path_to_application> chmod 755 * harp <pipeline-configration>.json The results of the Framework are stored in the predictions.json file under the target application folder. Please find the sample application under the example folder and follow the readme file to execute the Framework against profiling and estimating the resource needs. Releases The current release is 2.0.0 . Supported Systems System Name Version V1 Support Version V2 Support CPU GPU Local Linux machines :white_check_mark: :white_check_mark: :white_check_mark: Owens (OSC) :white_check_mark: :white_check_mark: :white_check_mark: :white_check_mark: Pitzer (OSC) :white_check_mark: :white_check_mark: :white_check_mark: :white_check_mark: Frontera (TACC) :white_check_mark: :white_check_mark: :white_check_mark: Stampede2 (TACC) :white_check_mark: :white_check_mark: :white_check_mark: Citing HARP Please cite the following paper if using HARP: S. Vallabhajosyula and R. Ramnath, \"Establishing a Generalizable Framework for Generating Cost-Aware Training Data and Building Unique Context-Aware Walltime Prediction Regression Models,\" 2022 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom), Melbourne, Australia, 2022, pp. 497-506, doi: 10.1109/ISPA-BDCloud-SocialCom-SustainCom57177.2022.00070. Other papers: Vallabhajosyula, Manikya Swathi, and Rajiv Ramnath. \"Towards Practical, Generalizable Machine-Learning Training Pipelines to build Regression Models for Predicting Application Resource Needs on HPC Systems.\" Practice and Experience in Advanced Research Computing. 2022. 1-5. Reporting Bugs and Contribution Please open an issue on the github issues page to report a bug or email vallabhajosyula.2@buckeyemail.osu.edu (with subject \"HARP GitHub\") HARP is an open-source repository, and we invite the community to collaborate and include their workflows into the Framework to profile their applications. Create a pull request to add your changes to the dev branch. SUBSCRIBE to ICICLE discussion mailing list at https://icicle.osu.edu/engagement/mailing-lists License The HARP is licensed under the https://opensource.org/licenses/BSD-3-Clause # Acknowledgements This work has been funded by grants from the National Science Foundation, including the ICICLE AI Institute (OAC 2112606) and EAGER (OAC 1945347)","title":"Index"},{"location":"harp/#harp-hpc-application-runtime-predictor","text":"","title":"HARP - HPC Application Runtime Predictor"},{"location":"harp/#overview","text":"Researchers use high-performance computing (HPC) cyberinfrastructures (CI) like the Ohio Supercomputer (OSC) or Texas Advanced Computing Center (TACC) to execute computationally intensive diverse scientific workflows. Some workflows are heavy on IO, like genome sequencing (cleaning and assembly), while others, like training DNNs, could be compute (and memory) intensive. Each workflow has a unique resource requirement, and it is essential to profile and understand these needs to allocate shared resources for optimal utilization of the cyberinfrastructure. These resources are expensive, and several jobs compete to get these allocations, sometimes with reasonable wait times (while requesting enormous resources for a long time). Estimating the expected resources for optimally utilizing the compute and memory is challenging, especially considering the need for sufficient history to enable these predictions tailored for unique workflows and execution environments. We explored and established a framework (as shown in Figure 1) that pipelines the solutions to address these challenges. The Framework is configured to generate a history of executions and train suitable regression models to estimate the approximate execution time for a targeted application. Figure 1: The Proposed Framework: training data generation, building regression models, selecting the best model based on custom criteria","title":"Overview"},{"location":"harp/#components-and-characteristics-of-the-framework-from-figure-1","text":"Generating and Preparing Training Data: This module automatically and systematically generates comprehensive, diverse \"scaled-down(SD)\" and limited, selective \"full-scale(FS)\" runs with minimal human intervention. We use Cheetah (https://github.com/CODARcode/cheetah) to execute the target application with the pre-defined data generation configurations (SD and FS) to generate the history-of-runs training data. Building Regression Models: This module standardizes and prepares the data, trains the selected off-the-shelf regression models with the appropriate hyper-parameters, and stores them for inference. In this phase, the data generated in the first phase is processed to train regression models. Redundant features are eliminated, outliers are removed, and features are transformed to reduce the dimensionality before training the regression models. Selecting Appropriate Prediction Model: This module selects the most appropriate regression model from a pool of trained models from phase 2 with respect to a given policy and target application Note: The Framework is built on TensorFlow Framework. Figure 2: Shows the target-application execution endpoint and the harp application folder structure.","title":"Components and Characteristics of the Framework (from Figure 1):"},{"location":"harp/#ways-to-configure-harp-to-setup-applications-for-profiling","text":"Configure HARP with TAPIS to execute HARP and applications to profile as container images. Install HARP as a loadable module on OSC or localbox.","title":"WAYS to configure HARP to setup applications for profiling:"},{"location":"harp/#way-1-configure-harp-with-tapis-to-configure-harp-and-applications-to-profile-as-container-images","text":"NOTE: IMPORTANT INFORMATION - PLEASE READ 1. TAPIS executes only containerized applications, so HARP (HARP framework) and the Application should be containerized. 2. Profiling an application HARP Framework with TAPIS is tested on both TACC and OSC systems. 3. Colab Notebooks server as examples for running HARP profiling for a sample Euler application using TAPIS. Make a COPY of the colab or download the Notebooks to execute them. 4. HARP Framework and example containers could be executed on local box user docker or apptaner servers without TAPIS integration. Follow through the Colab notebooks for profiling the HARP Container application using TAPIS: 1. For TACC stampede2 example use: https://drive.google.com/file/d/1JyAHUxxZ3pKMXGs28UXMQJn5QZmti1yS/view?usp=sharing 2. For OSC pitzer example use:https://drive.google.com/file/d/1w8qCTWiOjvn8CCx6FqvZzG8ZJBRKy4M3/view?usp=sharing Alternatively, download the example notebook from the \"Notebooks\" folder Steps: 1.Create a new HARP Image [or] using the pre-made HARP Image 2.Create an image for the application to be profiled using HARP Framework (Image) 3.Refer to Section \" Using HARP to profile an application and predict the execution time \" for steps to execute the container in localbox or a CI (like OSC or TACC nodes) with or without TAPIS. Create a HARP Image using Docker Environment a. Use the 'Dockerfile_HARP_local' file to create an Image for executing the Framework on a local box using 'docker build'. docker build -f DockerFiles/Dockerfile_HARP_local -t harp-framework-local:2.0.0 . b. Use the Dockerfile_HARP_CI file to create an Image for executing the Framework on CI (like TACC or OSC) using 'docker build'. docker build -f DockerFiles/Dockerfile_HARP_CI -t harp-framework-ci:2.0.0 . c. Push the Image to the docker hub or upload it to any web-accessible location. Push the image to a web-accessible location like dockerhub using the following command: docker push <DockerHub>/harp-framework-[local|ci]:2.0.0 If you already have access to the pre-compiled images, use these copies of the HARP image from our repository a. Use the harp-framework-local:2.0.0 file for executing the Framework on a local box. b. Use the harp-framework-ci:2.0.0 file for executing the Framework on TACC or OSC Systems. Note : Execute the 'docker build' command from the main folder 'harp'. Create an image for the application to be profiled using HARP Framework (Image) Using the Image created in \"Step 1\" or the respective Image from the ICICLE repository, create an Image for the application to be profiled using the HARP Framework. We refer to the application to be profiled as a 'target application'. Steps for creating a DockeFile for the target application and building an image using the 'Dockerfile_App_Template' template. a. Edit the 'ProfileApplication.sh' entry point file to execute the HARP with the pipeline configurations JSON. Replace \"pipeline_config.json\" with your desired pipeline configuration file in the application work folder. harp pipeline_config.json b. Create a DockerFile for the target application from the following template FROM <web-accessible-path-to-image-hub>/harp-framework-[local|ci]:2.0.0 # 1. Add application required installations. # none for Euler example # 2. Target application setup # Set the APP_PATH to the location with your application work folder (target application to be profiled) and APP_NAME to the work folder name ENV APP_PATH=\"<path-to-target-application>/<target-application-work-folder>\" ENV APP_NAME=\"<target-application-work-folder>\" # Add the target application work folder to the Image ADD $APP_PATH /app/$APP_PATH # 3. Copy the execution endpoint file and set it COPY DockerFiles/ProfileApplication.sh /app/ProfileApplication.sh ENTRYPOINT [\"sh\", \"/app/ProfileApplication.sh\"] c. Build the application Image (these commands show building an image for an example application 'Euler Number') a. Use the 'Dockerfile_App_EulerNumber' file to create an Image for executing the Framework on a local box using 'docker build'. docker build -f DockerFiles/Dockerfile_App_EulerNumber -t harp-app-eulernumber-[local|ci]:2.0.0 . b. Push the Image to the docker hub or upload it to any web-accessible location. Push the image to a web-accessible location like dockerhub using the following command: docker push <DockerHub>/harp-app-eulernumber-[local|ci]:2.0.0 Use an existing copy of the Euler Number application image from our repository. Refer to Section \" Using HARP to profile an application and predict the execution time \" for steps to execute the container in localbox or a CI (like OSC or TACC nodes) with or without TAPIS.","title":"WAY 1. Configure HARP with TAPIS to configure HARP and applications to profile as container images."},{"location":"harp/#way-2-installation-based-harp-setup","text":"Dependency: Linux, Python 3.9+, git, pip, mpich, psutil, jq(command line JSON parser https://stedolan.github.io/jq/) On supercomputers (OSC), it should be installed at a location accessible from the parallel file system","title":"WAY 2. Installation-based HARP setup"},{"location":"harp/#follow-these-steps-to-set-up-harp-as-a-loadable-software-module-on-the-ohio-supercomputer-osc","text":"bash git clone https://github.com/ICICLE-ai/harp.git cd harp chmod 755 install-osc-harp.sh ./install-osc-harp.sh If the installation fails, please re-run the script 'install-osc-harp.sh' after deleting the environment 'harp-env' and running the cleanup.sh in the install directory. bash conda remove --name harp-env --all ./cleanup.sh This setup installs miniconda, CODAR Cheetah (https://github.com/CODARcode/cheetah), TensorFlow, psutil, pandas, and scikit-learn and configures the Harp framework. Please follow the installation prompts to go ahead with the setup. This installation takes 30-40 mins to finish the setup on Owens login node.","title":"Follow these steps to set up HARP as a loadable software module on the Ohio Supercomputer (OSC):"},{"location":"harp/#loading-the-harp-module-on-osc","text":"bash module use $HOME/osc_apps/lmodfiles module load harp export CONDA_HOME=<path_to_miniconda>/miniconda3 source $CONDA_HOME/bin/activate source activate harp_env NOTE Things to consider while installing the Framework on OSC 1. [OSC Installation] The installer creates a conda environment, \"harp_env\" on OSC and uses this environment to execute the Framework. The environment name is used in a couple of Cheetash configurations and hence is mandated to use the same name, \"harp_env,\" while installing the application. Please delete the environment if it already exists with this name before installing the Framework. 2. Upon successful installation, the install script will return the below response: (OSC Install Script) Generating Module File Step: /users/PAS0536/swathivm/osc_apps/lmodfiles/harp/1.0.lua (OSC Install Script) Generating Module File Step Finished Finished at Thu Mar 16 11:44:13 EDT 2023 Execution time: 1965 seconds","title":"Loading the HARP module on OSC"},{"location":"harp/#follow-these-steps-to-set-the-harp-framework-on-a-standalone-linux-system","text":"Use these commands to install the dependencies using pip bash pip install psutil pip install tensorflow pip install pandas pip install scikit-learn Download the source code into the and set it to HARP_HOME bash git clone https://github.com/ICICLE-ai/harp.git export HARP_HOME=<path-to-download-folder>/harp Install Cheetah bash cd $HARP_HOME/cheetah pip install --editable . Ensure the scripts have ' execute ' privileges bash cd $HARP_HOME/pipeline/bin/local chmod 755 harp cd $HARP_HOME/cheetah/bin chmod 755 * Set the HARP pipeline and Cheetah binaries in the PATH bash export PATH=$HARP_HOME/pipeline/bin/local:$HARP_HOME/cheetah/bin:$PATH The HARP pipeline is ready to be used once the HARP_HOME and binaries are set in PATH. NOTE HARP has been tested on Ownes and Pitzer (OSC) and a standalone Linux system. Things to consider while installing the dependencies on standalone Linux systems: 1. if you do not have root or admin privileges, please consult your package manager on installing mpich and operator dependencies.","title":"Follow these steps to set the HARP framework on a standalone Linux system:"},{"location":"harp/#using-harp-to-profile-an-application-and-predict-the-execution-time","text":"","title":"Using HARP to profile an application and predict the execution time"},{"location":"harp/#using-the-harp-version-200-to-profile-an-application-eg-euler-number-on-local-box-and-cis-osc-and-tacc-using-tapis","text":"a. To profile the application, execute the application image built from the HARP parent image using the following commands on the localbox: [optional] docker pull ghcr.io/icicle-ai/harp-app-eulernumber-local:2.0.0 docker run --mount source=HARP_Store,target=/scratch ghcr.io/icicle-ai/harp-app-eulernumber-local:2.0.0 b. To profile the application using HARP on OSC or TACC i. Without TAPIS: Login into a compute node on OSC or TACC and run the following:","title":"Using the HARP (version 2.0.0) to profile an application (e.g., Euler Number) on local box and CIs (OSC and TACC) using TAPIS"},{"location":"harp/#for-osc-nodes","text":"module load singularity [optional] singularity pull docker://ghcr.io/icicle-ai/harp-app-eulernumber-ci:2.0.0 singularity run docker://ghcr.io/icicle-ai/harp-app-eulernumber-ci:2.0.0 osc /fs/scratch/PAS2271/swathivm/","title":"for OSC Nodes"},{"location":"harp/#for-tacc-nodes-stampede2","text":"module load tacc-apptainer [optional] singularity pull docker://ghcr.io/icicle-ai/harp-app-eulernumber-ci:2.0.0 singularity run docker://ghcr.io/icicle-ai/harp-app-eulernumber-ci:2.0.0 tacc none ii. With TAPIS: Follow the instructions in the ' Executing_HARP_using_TAPIS.ipynb ' notebook in the 'Notebooks' folder to register OSC and TACC systems on TAPIS and profile the application 'harp-app-eulernumber-ci:2.0.0' using HARP","title":"For TACC Nodes (Stampede2)"},{"location":"harp/#using-the-harp-version-100-module-load-the-osc-of-the-harp-binary-on-the-local-box","text":"Navigate to the target application folder and copy all the files from /Post_Execution_Scripts/basic into the current folder. Edit path in post-script.sh to point to the target application directory Execute the Framework as per the configurations in file 'train_config.json' as follows: bash cd <path_to_application> chmod 755 * harp <pipeline-configration>.json The results of the Framework are stored in the predictions.json file under the target application folder. Please find the sample application under the example folder and follow the readme file to execute the Framework against profiling and estimating the resource needs.","title":"Using the HARP (version 1.0.0) module, load the OSC of the HARP binary on the local box"},{"location":"harp/#releases","text":"The current release is 2.0.0 .","title":"Releases"},{"location":"harp/#supported-systems","text":"System Name Version V1 Support Version V2 Support CPU GPU Local Linux machines :white_check_mark: :white_check_mark: :white_check_mark: Owens (OSC) :white_check_mark: :white_check_mark: :white_check_mark: :white_check_mark: Pitzer (OSC) :white_check_mark: :white_check_mark: :white_check_mark: :white_check_mark: Frontera (TACC) :white_check_mark: :white_check_mark: :white_check_mark: Stampede2 (TACC) :white_check_mark: :white_check_mark: :white_check_mark:","title":"Supported Systems"},{"location":"harp/#citing-harp","text":"Please cite the following paper if using HARP: S. Vallabhajosyula and R. Ramnath, \"Establishing a Generalizable Framework for Generating Cost-Aware Training Data and Building Unique Context-Aware Walltime Prediction Regression Models,\" 2022 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom), Melbourne, Australia, 2022, pp. 497-506, doi: 10.1109/ISPA-BDCloud-SocialCom-SustainCom57177.2022.00070. Other papers: Vallabhajosyula, Manikya Swathi, and Rajiv Ramnath. \"Towards Practical, Generalizable Machine-Learning Training Pipelines to build Regression Models for Predicting Application Resource Needs on HPC Systems.\" Practice and Experience in Advanced Research Computing. 2022. 1-5.","title":"Citing HARP"},{"location":"harp/#reporting-bugs-and-contribution","text":"Please open an issue on the github issues page to report a bug or email vallabhajosyula.2@buckeyemail.osu.edu (with subject \"HARP GitHub\") HARP is an open-source repository, and we invite the community to collaborate and include their workflows into the Framework to profile their applications. Create a pull request to add your changes to the dev branch. SUBSCRIBE to ICICLE discussion mailing list at https://icicle.osu.edu/engagement/mailing-lists","title":"Reporting Bugs and Contribution"},{"location":"harp/#license","text":"The HARP is licensed under the https://opensource.org/licenses/BSD-3-Clause # Acknowledgements This work has been funded by grants from the National Science Foundation, including the ICICLE AI Institute (OAC 2112606) and EAGER (OAC 1945347)","title":"License"},{"location":"harp/cheetah/","text":"Cheetah - An Experiment Harness and Campaign Management System Overview Cheetah is an experiment harness for running codesign experiments to study the effects of online data analysis at the exascale. It provides a way to run large campaigns of experiments to understand the advantages and tradeoffs of different compression and reduction algorithms run using different orchestration mechanisms. Experiments can be run to analyze data offline, in situ (via a function that is part of the application), or online (in a separate, stand-alone application). The workflow may be composed so that different executables reside on separate nodes, or share compute nodes, in addition to fine-tuning the number of processes per node. Users create a campaign specification file in Python that describes the applications that form the workflow, and the parameters that they are interested in exploring. Cheetah creates the campaign endpoint on the target machine, and users can then launch experiments using the generated submission script. Cheetah's runtime framework, Savanna , translates experiment metadata into scheduler calls for the underlying system and manages the allocated resources for running experiments. Savanna contains definitions for different supercomputers; based upon this information about the target machine, Savanna uses the appropriate scheduler interface ( aprun , jsrun , slurm ) and the corresponding scheduler options to launch experiments. Cheetah is centered around ADIOS , a middleware library that provides an I/O framework along with a publish-subscribe API for exchanging data in memory. Typically, all ADIOS-specific settings are set in an XML file that is read by the application. Cheetah provides an interface to edit ADIOS XML files to tune I/O options. Installation Dependency: Linux, Python 3.5+, psutil On supercomputers it should be installed at a location accessible from the parallel file system Cheetah can be installed via the Spack package manager as spack install codar-cheetah@develop . Users can also download Cheetah and set the PATH: bash git clone git@github.com:CODARcode/cheetah.git cd cheetah python3 -m venv venv-cheetah source venv-cheetah/bin/activate pip install --editable . * Cheetah has been tested on Summit (ORNL), Andes (ORNL), Theta (ANL), Cori (LBNL), and standalone Linux computers Setting up a Cheetah environment bash source <cheetah dir>/venv-cheetah/bin/activate Documentation The recommended start is to go through the Cheetah Tutorial under docs/Tutorials. The Cheetah documentation can be found at https://codarcode.github.io/cheetah . Releases The current release is 1.1.2 . Supported Systems System Name Cheetah Support System supports Node-Sharing Cheetah Node-Sharing Support Local Linux machines :white_check_mark: N/A N/A Summit (ORNL) :white_check_mark: :white_check_mark: :white_check_mark: Andes (ORNL) :white_check_mark: :x: :x: Spock (ORNL) :white_check_mark: -- -- Theta (ANL) :white_check_mark: :x: N/A Cori (LBNL) :white_check_mark: :white_check_mark: In progress Authors The primary authors of Cheetah are Kshitij Mehta (ORNL) and Bryce Allen (University of Chicago). All contributors are listed here . Citing Cheetah To refer to Cheetah in a publication, please cite the following paper: Mehta, Kshitij, Allen, Bryce, Wolf, Matthew, Logan, Jeremy, Suchyta, Eric, Singhal, Swati, Choi, Jong Youl, Takahashi, Keichi, Huck, Kevin, Yakushin, Igor, Sussman, Alan, Munson, Todd, Foster, Ian, and Klasky, Scott. \"A codesign framework for online data analysis and reduction\" Journal: Concurrency and Computation: Practice and Experience https://doi.org/10.1002/cpe.6519. Other paper: * K. Mehta et al., \"A Codesign Framework for Online Data Analysis and Reduction,\" 2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS), Denver, CO, USA, 2019, pp. 11-20. doi: 10.1109/WORKS49585.2019.00007 URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8943548&isnumber=8943488 Examples For more examples of using Cheetah, see the examples directory. Calculating Euler's number Using Cheetah to run coupled applications Brusselator The Gray-Scott Reaction-Diffusion Benchmark Gray-Scott with compression, Z-Checker and FTK Contributing Cheetah is open source and we invite the community to collaborate. Create a pull-request to add your changes to the dev branch. Reporting Bugs Please open an issue on the github issues page to report a bug. License Cheetah is licensed under the Apache License v2.0. See the accompanying Copyright.txt for more details.","title":"Index"},{"location":"harp/cheetah/#cheetah-an-experiment-harness-and-campaign-management-system","text":"","title":"Cheetah - An Experiment Harness and Campaign Management System"},{"location":"harp/cheetah/#overview","text":"Cheetah is an experiment harness for running codesign experiments to study the effects of online data analysis at the exascale. It provides a way to run large campaigns of experiments to understand the advantages and tradeoffs of different compression and reduction algorithms run using different orchestration mechanisms. Experiments can be run to analyze data offline, in situ (via a function that is part of the application), or online (in a separate, stand-alone application). The workflow may be composed so that different executables reside on separate nodes, or share compute nodes, in addition to fine-tuning the number of processes per node. Users create a campaign specification file in Python that describes the applications that form the workflow, and the parameters that they are interested in exploring. Cheetah creates the campaign endpoint on the target machine, and users can then launch experiments using the generated submission script. Cheetah's runtime framework, Savanna , translates experiment metadata into scheduler calls for the underlying system and manages the allocated resources for running experiments. Savanna contains definitions for different supercomputers; based upon this information about the target machine, Savanna uses the appropriate scheduler interface ( aprun , jsrun , slurm ) and the corresponding scheduler options to launch experiments. Cheetah is centered around ADIOS , a middleware library that provides an I/O framework along with a publish-subscribe API for exchanging data in memory. Typically, all ADIOS-specific settings are set in an XML file that is read by the application. Cheetah provides an interface to edit ADIOS XML files to tune I/O options.","title":"Overview"},{"location":"harp/cheetah/#installation","text":"Dependency: Linux, Python 3.5+, psutil On supercomputers it should be installed at a location accessible from the parallel file system Cheetah can be installed via the Spack package manager as spack install codar-cheetah@develop . Users can also download Cheetah and set the PATH: bash git clone git@github.com:CODARcode/cheetah.git cd cheetah python3 -m venv venv-cheetah source venv-cheetah/bin/activate pip install --editable . * Cheetah has been tested on Summit (ORNL), Andes (ORNL), Theta (ANL), Cori (LBNL), and standalone Linux computers","title":"Installation"},{"location":"harp/cheetah/#setting-up-a-cheetah-environment","text":"bash source <cheetah dir>/venv-cheetah/bin/activate","title":"Setting up a Cheetah environment"},{"location":"harp/cheetah/#documentation","text":"The recommended start is to go through the Cheetah Tutorial under docs/Tutorials. The Cheetah documentation can be found at https://codarcode.github.io/cheetah .","title":"Documentation"},{"location":"harp/cheetah/#releases","text":"The current release is 1.1.2 .","title":"Releases"},{"location":"harp/cheetah/#supported-systems","text":"System Name Cheetah Support System supports Node-Sharing Cheetah Node-Sharing Support Local Linux machines :white_check_mark: N/A N/A Summit (ORNL) :white_check_mark: :white_check_mark: :white_check_mark: Andes (ORNL) :white_check_mark: :x: :x: Spock (ORNL) :white_check_mark: -- -- Theta (ANL) :white_check_mark: :x: N/A Cori (LBNL) :white_check_mark: :white_check_mark: In progress","title":"Supported Systems"},{"location":"harp/cheetah/#authors","text":"The primary authors of Cheetah are Kshitij Mehta (ORNL) and Bryce Allen (University of Chicago). All contributors are listed here .","title":"Authors"},{"location":"harp/cheetah/#citing-cheetah","text":"To refer to Cheetah in a publication, please cite the following paper: Mehta, Kshitij, Allen, Bryce, Wolf, Matthew, Logan, Jeremy, Suchyta, Eric, Singhal, Swati, Choi, Jong Youl, Takahashi, Keichi, Huck, Kevin, Yakushin, Igor, Sussman, Alan, Munson, Todd, Foster, Ian, and Klasky, Scott. \"A codesign framework for online data analysis and reduction\" Journal: Concurrency and Computation: Practice and Experience https://doi.org/10.1002/cpe.6519. Other paper: * K. Mehta et al., \"A Codesign Framework for Online Data Analysis and Reduction,\" 2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS), Denver, CO, USA, 2019, pp. 11-20. doi: 10.1109/WORKS49585.2019.00007 URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8943548&isnumber=8943488","title":"Citing Cheetah"},{"location":"harp/cheetah/#examples","text":"For more examples of using Cheetah, see the examples directory. Calculating Euler's number Using Cheetah to run coupled applications Brusselator The Gray-Scott Reaction-Diffusion Benchmark Gray-Scott with compression, Z-Checker and FTK","title":"Examples"},{"location":"harp/cheetah/#contributing","text":"Cheetah is open source and we invite the community to collaborate. Create a pull-request to add your changes to the dev branch.","title":"Contributing"},{"location":"harp/cheetah/#reporting-bugs","text":"Please open an issue on the github issues page to report a bug.","title":"Reporting Bugs"},{"location":"harp/cheetah/#license","text":"Cheetah is licensed under the Apache License v2.0. See the accompanying Copyright.txt for more details.","title":"License"},{"location":"harp/cheetah/tests/","text":"To run locally make run_local To check the status make status_local Once the status is DONE , run tests: make test_local The performed tests are: use Cheetah to generate report, count the number of succeeded and failed jobs according to the report, compare stdout from each job with the reference, excluding time(s) = line that might be different, check for non-empty stderr files for each job. Similary, one can substitute local by summit , theta or other supported supercomputer.","title":"Index"},{"location":"harp/cheetah/tests/build_summit/","text":"How to build the dependencies on Summit Our attempt to use spack on Summit to build all the dependencies so far failed and for now we are doing it manually. Load the following modules module load gcc/7.4.0 module load zeromq/4.2.5 module load libfabric/1.7.0 module load python/3.7.0-anaconda3-5.3.0 Create conda environment conda create -n Test conda install conda pip numpy scipy pandas matplotlib source activae Test Build mpi4py from source (using conda install is not a good idea since we want to make sure that system MPI is used) wget https://bitbucket.org/mpi4py/mpi4py/downloads/mpi4py-3.0.3.tar.gz tar xvf mpi4py-3.0.3.tar.gz cd mpi4py-3.0.3 python setup.py install Install Cheetah/Savanna git clone git@github.com:CODARcode/cheetah.git cd cheetah python setup.py install Build simplest ADIOS installation with python support git clone https://github.com/ornladios/ADIOS2.git ADIOS2 cd ADIOS2 mkdir build cd build cmake -DCMAKE_CXX_COMPILER=`which g++` -DCMAKE_C_COMPILER=`which gcc` -DCMAKE_Fortran_COMPILER:FILEPATH=`which gfortran` \\ -DADIOS2_USE_MPI:STRING=True -DCMAKE_INSTALL_PREFIX=$HOME/SOFT_INSTALL/adios-2.5_simplest-gcc-7.4.0 -DADIOS2_USE_HDF5:STRING=False -DADIOS2_USE_SZ:STRING=False -DADIOS2_USE_ZFP:STRING=False -DADIOS2_USE_Blosc:STRING=False -DADIOS2_USE_BZip2:STRING=False -DADIOS2_USE_PNG:STRING=False -DADIOS2_USE_DataMan:STRING=True -DADIOS2_USE_Fortran:STRING=False .. make -j20 make install Create a module (lua file) to use ADIOS, for example: ``` dir = os.getenv(\"HOME\") .. '/SOFT_INSTALL/adios-2.5_simplest-gcc-7.4.0/' prepend_path('PATH', dir .. 'bin') prepend_path('LD_LIBRARY_PATH', dir .. 'lib64') prepend_path('CPATH', dir .. 'include') prepend_path('PYTHONPATH', dir .. 'lib64/python3.6/site-packages') setenv('ADIOS2_DIR', dir) * Use `module use ...` to make the corresponding directory with modules visible and load ADIOS, for example, as follows: module load Compiler/gcc/7.4.0/adios2/2.5_simplest-gcc-7.4.0 * Launch python interpreter and test that you can import adios2 import cheetah import mpi4py import numpy import pandas ``` * If it worked, you are ready to run tests","title":"How to build the dependencies on Summit"},{"location":"harp/cheetah/tests/build_summit/#how-to-build-the-dependencies-on-summit","text":"Our attempt to use spack on Summit to build all the dependencies so far failed and for now we are doing it manually. Load the following modules module load gcc/7.4.0 module load zeromq/4.2.5 module load libfabric/1.7.0 module load python/3.7.0-anaconda3-5.3.0 Create conda environment conda create -n Test conda install conda pip numpy scipy pandas matplotlib source activae Test Build mpi4py from source (using conda install is not a good idea since we want to make sure that system MPI is used) wget https://bitbucket.org/mpi4py/mpi4py/downloads/mpi4py-3.0.3.tar.gz tar xvf mpi4py-3.0.3.tar.gz cd mpi4py-3.0.3 python setup.py install Install Cheetah/Savanna git clone git@github.com:CODARcode/cheetah.git cd cheetah python setup.py install Build simplest ADIOS installation with python support git clone https://github.com/ornladios/ADIOS2.git ADIOS2 cd ADIOS2 mkdir build cd build cmake -DCMAKE_CXX_COMPILER=`which g++` -DCMAKE_C_COMPILER=`which gcc` -DCMAKE_Fortran_COMPILER:FILEPATH=`which gfortran` \\ -DADIOS2_USE_MPI:STRING=True -DCMAKE_INSTALL_PREFIX=$HOME/SOFT_INSTALL/adios-2.5_simplest-gcc-7.4.0 -DADIOS2_USE_HDF5:STRING=False -DADIOS2_USE_SZ:STRING=False -DADIOS2_USE_ZFP:STRING=False -DADIOS2_USE_Blosc:STRING=False -DADIOS2_USE_BZip2:STRING=False -DADIOS2_USE_PNG:STRING=False -DADIOS2_USE_DataMan:STRING=True -DADIOS2_USE_Fortran:STRING=False .. make -j20 make install Create a module (lua file) to use ADIOS, for example: ``` dir = os.getenv(\"HOME\") .. '/SOFT_INSTALL/adios-2.5_simplest-gcc-7.4.0/' prepend_path('PATH', dir .. 'bin') prepend_path('LD_LIBRARY_PATH', dir .. 'lib64') prepend_path('CPATH', dir .. 'include') prepend_path('PYTHONPATH', dir .. 'lib64/python3.6/site-packages') setenv('ADIOS2_DIR', dir) * Use `module use ...` to make the corresponding directory with modules visible and load ADIOS, for example, as follows: module load Compiler/gcc/7.4.0/adios2/2.5_simplest-gcc-7.4.0 * Launch python interpreter and test that you can import adios2 import cheetah import mpi4py import numpy import pandas ``` * If it worked, you are ready to run tests","title":"How to build the dependencies on Summit"},{"location":"harp/cheetah/tests/build_theta/","text":"How to build dependencies on Theta","title":"How to build dependencies on Theta"},{"location":"harp/cheetah/tests/build_theta/#how-to-build-dependencies-on-theta","text":"","title":"How to build dependencies on Theta"},{"location":"harp/examples/01-eulers_number/","text":"Estimating walltime to execute \"calc_e.py\" with given configurations for application: Calculating Euler's number This code is from \"https://github.com/CODARcode/cheetah\" To demonstrate the usage of HARP with Cheetah, we will look at a simple Python3 script that calculates Euler's number ( e ) using two different methods. It takes three positional arguments: the first is a string describing which of the methods to use, the second is a parameter to the calculation method that describes how many iterations or how precise to try to make the calculation (meaning depends on the method), and the third is a precision to use with the standard library Decimal module in python. If the third argument is not specified, built-in floating point is used instead. The application The source code is contained in a single source file: calc_e.py . There are no dependencies other than python3. How to use HARP to estimate the walltime for the program calc_e.py? The current folder '01-eulers_number' is called target application folder. Navigate to the target application folder and copy the all the files from /Post_Execution_Scripts/basic into the the current folder. For more details about the type of application categories and profiling, please read the document or PPT . Edit the paths in \"post_script.sh\" to point to this folder (the target application folder). Edit the following files before applying the HARP framework to the application If running the pipeline on OSC, set the OSC project account in the campaign files \" campaign .py\". Ignore otherwise. class GrayScott(Campaign): ... scheduler_options = {'owens_gpu': {'project':'<OSC-Project-Account>'}} ... Adjust the paths for the cheetah_app_directory and ((cheetah_campaign_file** keys in pipeline_config.json file to pointto the current directory. Run the following commands to set/check the environment to run HARP framework To load the module and set the environment on OSC (either in the command line mode or in the sbatch script) bash module use $HOME/osc_apps/lmodfiles module load harp export CONDA_HOME=<path-to-conda-install>/miniconda3 source $CONDA_HOME/bin/activate source activate harp_env For running harp on Standalone Linux System Ensure that HARP_HOME environment variable is set to the HARP install directory and the Cheetah and HARP binaries are in PATH bash echo $HARP_HOME echo $PATH Run the following commands to execute the framework from the target application folder cd <path-to-01-eulers_number- folder> chmod 755 * harp pipeline_config.json Before executing the harp, make sure to modify the absolute-paths in 'pipeline_config.json' file to the target application folder i.e., this example folder. The framework performs the following operations Generates training data with three different configurations (scaled-down, full-scale, and test-data), copies it to the pipeline/applications/ /train folder for further processing It pre-processes the data and transforms the training data using principal component analysis, upsamples full-scale executions to match scaled-down training samples, and trains and stores the regression models with different configurations The regression model with better predictions (lower no. of under-predictions and lower MAPE of over-estimations) suitable for Euler training data is selected and is used to predict the walltime estimations for the test-data configurations. The results of the framework are stored in predictions.csv in the target application folder.","title":"Estimating walltime to execute \"calc_e.py\" with given configurations for application: Calculating Euler's number"},{"location":"harp/examples/01-eulers_number/#estimating-walltime-to-execute-calc_epy-with-given-configurations-for-application-calculating-eulers-number","text":"This code is from \"https://github.com/CODARcode/cheetah\" To demonstrate the usage of HARP with Cheetah, we will look at a simple Python3 script that calculates Euler's number ( e ) using two different methods. It takes three positional arguments: the first is a string describing which of the methods to use, the second is a parameter to the calculation method that describes how many iterations or how precise to try to make the calculation (meaning depends on the method), and the third is a precision to use with the standard library Decimal module in python. If the third argument is not specified, built-in floating point is used instead.","title":"Estimating walltime to execute \"calc_e.py\" with given configurations for application: Calculating Euler's number"},{"location":"harp/examples/01-eulers_number/#the-application","text":"The source code is contained in a single source file: calc_e.py . There are no dependencies other than python3.","title":"The application"},{"location":"harp/examples/01-eulers_number/#how-to-use-harp-to-estimate-the-walltime-for-the-program-calc_epy","text":"The current folder '01-eulers_number' is called target application folder. Navigate to the target application folder and copy the all the files from /Post_Execution_Scripts/basic into the the current folder. For more details about the type of application categories and profiling, please read the document or PPT . Edit the paths in \"post_script.sh\" to point to this folder (the target application folder). Edit the following files before applying the HARP framework to the application If running the pipeline on OSC, set the OSC project account in the campaign files \" campaign .py\". Ignore otherwise. class GrayScott(Campaign): ... scheduler_options = {'owens_gpu': {'project':'<OSC-Project-Account>'}} ... Adjust the paths for the cheetah_app_directory and ((cheetah_campaign_file** keys in pipeline_config.json file to pointto the current directory. Run the following commands to set/check the environment to run HARP framework To load the module and set the environment on OSC (either in the command line mode or in the sbatch script) bash module use $HOME/osc_apps/lmodfiles module load harp export CONDA_HOME=<path-to-conda-install>/miniconda3 source $CONDA_HOME/bin/activate source activate harp_env For running harp on Standalone Linux System Ensure that HARP_HOME environment variable is set to the HARP install directory and the Cheetah and HARP binaries are in PATH bash echo $HARP_HOME echo $PATH Run the following commands to execute the framework from the target application folder cd <path-to-01-eulers_number- folder> chmod 755 * harp pipeline_config.json Before executing the harp, make sure to modify the absolute-paths in 'pipeline_config.json' file to the target application folder i.e., this example folder. The framework performs the following operations Generates training data with three different configurations (scaled-down, full-scale, and test-data), copies it to the pipeline/applications/ /train folder for further processing It pre-processes the data and transforms the training data using principal component analysis, upsamples full-scale executions to match scaled-down training samples, and trains and stores the regression models with different configurations The regression model with better predictions (lower no. of under-predictions and lower MAPE of over-estimations) suitable for Euler training data is selected and is used to predict the walltime estimations for the test-data configurations. The results of the framework are stored in predictions.csv in the target application folder.","title":"How to use HARP to estimate the walltime for the program calc_e.py?"},{"location":"harp/install-script-source/","text":"README This repo contains files to help users write short \"local install scripts\" to build, install, and manage software in their home directories. Typically, a user will want to copy install-osc_sample.sh and follow the notes within to get it to download, build, and install the desired software. Some additional description of the install script and template are provided below, as well as simple example at the bottom. If you have questions or need help writing an install script, feel free to contact OSC Help . Description install-template.sh is a template script with default/\"boilerplate\" functions to be sourced at the beginning of all user install scripts. install-osc_sample.sh is an example of an install script that a user would copy and modify to locally install some software that OSC does not provide. The expectation is that users will copy install-osc_sample.sh and modify it to suit their needs. Note that by default: - The download and build directories, as well as the log file, will be placed in the directory from which the install script is run - The software will be installed into $HOME/osc_apps/<pkgname>/<pkgversion> (configurable by making a modified config file) - The module file will be placed in $HOME/osc_apps/lmodfiles/<pkgname>/<pkgversion>.lua (configurable by making a modified config file) Assuming the default for the module file directory, the user will need to run module use $HOME/osc_apps/lmodfiles in any session in which they want to load their locally-installed software. Command-line options -c | --config <config_file> : Specify location of non-default config file to be used during installation Environment Variables Defined by the install script These environment variables may be optionally defined by the user in the install script and modify the behavior of the template file: - MODULE_SETTING : if only , skip installation and just update the module file; if none , then do not generate a module file - VERIFY_FILES : list of files that must be in the install directory for a valid installation, exit with error if any are missing. - KEEP_INSTALLDIR : if 1 , do not delete install directory at beginning of installation; otherwise, delete install directory Defined by the template These variables are defined by the template file and can be used by the user in the install script: - installdir : directory where application should be installed - srcdir : directory containing install script, build directory, and log files for application - moddir : directory module file for application should be located - modfile : module file to be created/updated - builddir : directory where application should be built - dldir : directory where files necessary for installation should be downloaded - pkgmaj : major version of application - pkgmin : minor version of application - pkgpatch : patch version of application - pkgmajmin : major and minor version of application - pkgbugfix : bug fix version of application - CC : C compiler - CXX : C++ compiler - FC : Fortran compiler - F77 : Fortran77 compiler - F90 : Fortran90 compiler - COMPILER_FAMILY : compiler family (i.e., GNU, Intel, or PGI) - MPI_CC : MPI C compiler - MPI_CXX : MPI Fortran compiler - MPI_FC : MPI Fortran compiler Defined by config file These variables are defined by the config file. Several (marked below as UNUSED) are not used for these local installs and are only kept in the config file for the purpose of consistency with our central install scripts. - system_name : UNUSED - topdir : top-level directory for installations (default ~/osc_apps ) - modtop : top-level directory for module files (default ~/osc_apps/lmodfiles ) - srctop : UNUSED (software is downloaded and built in the directory from which the install script is run) - update_cache_command : UNUSED - installdirlen : UNUSED - has_facls : UNUSED - add_facls : UNUSED A user can copy the provided config file and make changes as needed. In this case, the install script must be run with the -c|--config flag: ./install-osc.sh -c <path to modified config> Example For a simple example of a local installation using an install script, see the example subdirectory and the README.md within.","title":"README"},{"location":"harp/install-script-source/#readme","text":"This repo contains files to help users write short \"local install scripts\" to build, install, and manage software in their home directories. Typically, a user will want to copy install-osc_sample.sh and follow the notes within to get it to download, build, and install the desired software. Some additional description of the install script and template are provided below, as well as simple example at the bottom. If you have questions or need help writing an install script, feel free to contact OSC Help .","title":"README"},{"location":"harp/install-script-source/#description","text":"install-template.sh is a template script with default/\"boilerplate\" functions to be sourced at the beginning of all user install scripts. install-osc_sample.sh is an example of an install script that a user would copy and modify to locally install some software that OSC does not provide. The expectation is that users will copy install-osc_sample.sh and modify it to suit their needs. Note that by default: - The download and build directories, as well as the log file, will be placed in the directory from which the install script is run - The software will be installed into $HOME/osc_apps/<pkgname>/<pkgversion> (configurable by making a modified config file) - The module file will be placed in $HOME/osc_apps/lmodfiles/<pkgname>/<pkgversion>.lua (configurable by making a modified config file) Assuming the default for the module file directory, the user will need to run module use $HOME/osc_apps/lmodfiles in any session in which they want to load their locally-installed software.","title":"Description"},{"location":"harp/install-script-source/#command-line-options","text":"-c | --config <config_file> : Specify location of non-default config file to be used during installation","title":"Command-line options"},{"location":"harp/install-script-source/#environment-variables","text":"","title":"Environment Variables"},{"location":"harp/install-script-source/#defined-by-the-install-script","text":"These environment variables may be optionally defined by the user in the install script and modify the behavior of the template file: - MODULE_SETTING : if only , skip installation and just update the module file; if none , then do not generate a module file - VERIFY_FILES : list of files that must be in the install directory for a valid installation, exit with error if any are missing. - KEEP_INSTALLDIR : if 1 , do not delete install directory at beginning of installation; otherwise, delete install directory","title":"Defined by the install script"},{"location":"harp/install-script-source/#defined-by-the-template","text":"These variables are defined by the template file and can be used by the user in the install script: - installdir : directory where application should be installed - srcdir : directory containing install script, build directory, and log files for application - moddir : directory module file for application should be located - modfile : module file to be created/updated - builddir : directory where application should be built - dldir : directory where files necessary for installation should be downloaded - pkgmaj : major version of application - pkgmin : minor version of application - pkgpatch : patch version of application - pkgmajmin : major and minor version of application - pkgbugfix : bug fix version of application - CC : C compiler - CXX : C++ compiler - FC : Fortran compiler - F77 : Fortran77 compiler - F90 : Fortran90 compiler - COMPILER_FAMILY : compiler family (i.e., GNU, Intel, or PGI) - MPI_CC : MPI C compiler - MPI_CXX : MPI Fortran compiler - MPI_FC : MPI Fortran compiler","title":"Defined by the template"},{"location":"harp/install-script-source/#defined-by-config-file","text":"These variables are defined by the config file. Several (marked below as UNUSED) are not used for these local installs and are only kept in the config file for the purpose of consistency with our central install scripts. - system_name : UNUSED - topdir : top-level directory for installations (default ~/osc_apps ) - modtop : top-level directory for module files (default ~/osc_apps/lmodfiles ) - srctop : UNUSED (software is downloaded and built in the directory from which the install script is run) - update_cache_command : UNUSED - installdirlen : UNUSED - has_facls : UNUSED - add_facls : UNUSED A user can copy the provided config file and make changes as needed. In this case, the install script must be run with the -c|--config flag: ./install-osc.sh -c <path to modified config>","title":"Defined by config file"},{"location":"harp/install-script-source/#example","text":"For a simple example of a local installation using an install script, see the example subdirectory and the README.md within.","title":"Example"},{"location":"harp/install-script-source/example/","text":"Local Install Script Example This directory contains the files to install a very simple \"Hello World\" program into your home directory using a user/local install script. Example Software The tar file in this directory contains the source code and build files for a very simple software package. The package builds a single shared library, which defines a \"Hello World\" function, and a single executable, which links against the library and calls the function. Install Script In this directory you will find the file install-osc.sh . It was derived from /users/PZS0645/support/share/install-script/install-osc_sample.sh When you go about trying to \"locally\" install software, you can start by copying that sample file and modifying it to suit your purposes. In this case, we modified: - The initialize command: to have the software name (hello) and version (1.0) - VERIFY_FILES : to check that the hello executable is correctly installed - obtains_src , setup_step , configure_step , make_step , and make_install_step : to obtain and build our software (in general, you should read the documentation/README included with your software to determine what commands to execute in these steps; it is often some variation on wget , tar , ./configure --prefix=... , make , make install ) - generate_module_file : to make the module file set the necessary paths for our executable to function correctly For most software installs, you will need to make similar modifications to the sample file. Installation To install the hello package, copy the entire example directory and execute the install script. For example: $ cp ~support/share/install-script/example ~/example -r $ cd ~/example $ ./install-osc.sh [omitted output] This will install the software and module file under ~/osc_apps (creating the directory structure if it doesn't exist). You can now use the software by loading the module: $ module use ~/osc_apps/lmodfiles # execute each session to use all your module files $ module load hello $ hello Hello World!","title":"Local Install Script Example"},{"location":"harp/install-script-source/example/#local-install-script-example","text":"This directory contains the files to install a very simple \"Hello World\" program into your home directory using a user/local install script.","title":"Local Install Script Example"},{"location":"harp/install-script-source/example/#example-software","text":"The tar file in this directory contains the source code and build files for a very simple software package. The package builds a single shared library, which defines a \"Hello World\" function, and a single executable, which links against the library and calls the function.","title":"Example Software"},{"location":"harp/install-script-source/example/#install-script","text":"In this directory you will find the file install-osc.sh . It was derived from /users/PZS0645/support/share/install-script/install-osc_sample.sh When you go about trying to \"locally\" install software, you can start by copying that sample file and modifying it to suit your purposes. In this case, we modified: - The initialize command: to have the software name (hello) and version (1.0) - VERIFY_FILES : to check that the hello executable is correctly installed - obtains_src , setup_step , configure_step , make_step , and make_install_step : to obtain and build our software (in general, you should read the documentation/README included with your software to determine what commands to execute in these steps; it is often some variation on wget , tar , ./configure --prefix=... , make , make install ) - generate_module_file : to make the module file set the necessary paths for our executable to function correctly For most software installs, you will need to make similar modifications to the sample file.","title":"Install Script"},{"location":"harp/install-script-source/example/#installation","text":"To install the hello package, copy the entire example directory and execute the install script. For example: $ cp ~support/share/install-script/example ~/example -r $ cd ~/example $ ./install-osc.sh [omitted output] This will install the software and module file under ~/osc_apps (creating the directory structure if it doesn't exist). You can now use the software by loading the module: $ module use ~/osc_apps/lmodfiles # execute each session to use all your module files $ module load hello $ hello Hello World!","title":"Installation"},{"location":"hello_icicle_auth_clients/","text":"Hello ICICLE Authentication Clients Repo for Authenticated Clients and Applications for ICICLE CI Services Contents: * Overview * Software Releases * About the Team * Acknowledgement Overview The Artificial Intelligence (AI) institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment (ICICLE) is funded by the NSF to build the next generation of Cyberinfrastructure to render AI more accessible to everyone and drive its further democratization in the larger society. ICICLE aims to develop intelligent cyberinfrastructure with transparent and high-performance execution on diverse and heterogeneous environments as well as advance plug-and-play AI that is easy to use by scientists across a wide range of domains, promoting the democratization of AI. The deep AI infrastructure that ICICLE plans to utilize to sift through data relies on knowledge graphs (KGs) in which information is stored in a graph database that uses a graph-structured data model or data model to represent a network of entities and the relationships between them. Finding a way to make KGs easily accessible and functional on high performance computing (HPC) systems is an important step in helping to democratize HPC. Thus a large focus of this project was contributing to the body of knowledge needed for hosting live, dynamic, and interactive services that interface with HPC systems hosting KGs for ICICLE based resources and services In this project, we develop Jupyter Notebooks and Python command line clients that will access ICICLE resources and services using ICICLE authentication mechanisms. To connect our clients, we used Tapis, which is a framework that supports computational research to enable scientists to access and utilize and manage multi-institution resources and services. We used Neo4j to organize data into a knowledge graph (KG). We then hosted the KG on a Tapis Pod, which offers persistent data storage with a template made specifically for Neo4j KGs. Software Releases icicle_rel_03_2023 For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on any Tapis service. These applciations are stand-alone and can be installed separately. For details, see: * Jupyter Notebooks * Command Line Interfaces Command Line Applications Our CLI's are production software intended for use as interfaces to Tapis services hosted on HPC systems. These are ready to install and use, provided the proper requirements are fulfilled. Jupyter Notebooks The Jupyter notebooks in this repository are primarily demonstrators for working Tapis code, written in python. We also made an extensible template notebook which has Tapis auth prebuilt, and can be easily modified to carry out specific Tapis related tasks. The Notebooks and the CLIs each have their own directory and software requirements which are described here: About the Team This software was developed as part of the SDSC/UCSD Smmer 2022 REHS Project , titled Developing Interactive Jupyter Notebooks to run on the SDSC HPC Expanse System and the \u201cAI institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment\u201d (ICICLE) project. Project Lead: Mary Thomas, Ph.D., SDSC HPC Training lead, and Computational Data Scientist in the Data-Enabled Scientific Computing Division. REHS Students: Sahil Samar, Del Norte High School, San Diego, CA, sahilsamar031@gmail.com Mia Chen, Westview High School, San Diego, CA, mialunachen@gmail.com Jack Karpinski, San Diego High School, San Diego, CA, USA, jackadoo4@gmail.com Michael Ray, JSerra Catholic High School, San Juan Capistrano, CA, michael.ray@jserra.org Archita Sarin, Mission San Jose High School, Fremont, CA, archita.sarin@gmail.com Collaborators/Mentors: Christian Garcia, Engineering Scientist Associate (Texas Advanced Computing Center [5]). Matthew Lange, Ph.D., CEO, International Center for Food Ontology Operability Data and Semantics (IC-FOODS [4]); Joe Stubbs, Ph.D., Manager, Cloud & Interactive Computing (Texas Advanced Computing Center [5]). Acknowledgement This work has been funded by grants from the National Science Foundation, including: * The AI Institute for Intelligent CyberInfrastructure with Computational Learning in the Environment (ICICLE) ( #2112606 ) * The SDSC Expanse project ( #1928224 ) * The TACC Stampede System ( #1663578 ) * Tapis projects ( #1931439 ) * the NSF Track 3 Award: COre National Ecosystem for CyberinfrasTructure (CONECT) ( #2138307 ) and the Extreme Science and Engineering Discovery Environment (XSEDE) ( ACI-1548562 ). NOTES: YAML file: See Component Data Yaml file: https://github.com/ICICLE-ai/CI-Components-Catalog/blob/master/components-data.yaml","title":"Hello ICICLE Authentication Clients"},{"location":"hello_icicle_auth_clients/#hello-icicle-authentication-clients","text":"Repo for Authenticated Clients and Applications for ICICLE CI Services Contents: * Overview * Software Releases * About the Team * Acknowledgement","title":"Hello ICICLE Authentication Clients"},{"location":"hello_icicle_auth_clients/#overview","text":"The Artificial Intelligence (AI) institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment (ICICLE) is funded by the NSF to build the next generation of Cyberinfrastructure to render AI more accessible to everyone and drive its further democratization in the larger society. ICICLE aims to develop intelligent cyberinfrastructure with transparent and high-performance execution on diverse and heterogeneous environments as well as advance plug-and-play AI that is easy to use by scientists across a wide range of domains, promoting the democratization of AI. The deep AI infrastructure that ICICLE plans to utilize to sift through data relies on knowledge graphs (KGs) in which information is stored in a graph database that uses a graph-structured data model or data model to represent a network of entities and the relationships between them. Finding a way to make KGs easily accessible and functional on high performance computing (HPC) systems is an important step in helping to democratize HPC. Thus a large focus of this project was contributing to the body of knowledge needed for hosting live, dynamic, and interactive services that interface with HPC systems hosting KGs for ICICLE based resources and services In this project, we develop Jupyter Notebooks and Python command line clients that will access ICICLE resources and services using ICICLE authentication mechanisms. To connect our clients, we used Tapis, which is a framework that supports computational research to enable scientists to access and utilize and manage multi-institution resources and services. We used Neo4j to organize data into a knowledge graph (KG). We then hosted the KG on a Tapis Pod, which offers persistent data storage with a template made specifically for Neo4j KGs.","title":"Overview"},{"location":"hello_icicle_auth_clients/#software-releases","text":"","title":"Software Releases"},{"location":"hello_icicle_auth_clients/#icicle_rel_03_2023","text":"For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on any Tapis service. These applciations are stand-alone and can be installed separately. For details, see: * Jupyter Notebooks * Command Line Interfaces","title":"icicle_rel_03_2023"},{"location":"hello_icicle_auth_clients/#command-line-applications","text":"Our CLI's are production software intended for use as interfaces to Tapis services hosted on HPC systems. These are ready to install and use, provided the proper requirements are fulfilled.","title":"Command Line Applications"},{"location":"hello_icicle_auth_clients/#jupyter-notebooks","text":"The Jupyter notebooks in this repository are primarily demonstrators for working Tapis code, written in python. We also made an extensible template notebook which has Tapis auth prebuilt, and can be easily modified to carry out specific Tapis related tasks. The Notebooks and the CLIs each have their own directory and software requirements which are described here:","title":"Jupyter Notebooks"},{"location":"hello_icicle_auth_clients/#about-the-team","text":"This software was developed as part of the SDSC/UCSD Smmer 2022 REHS Project , titled Developing Interactive Jupyter Notebooks to run on the SDSC HPC Expanse System and the \u201cAI institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment\u201d (ICICLE) project. Project Lead: Mary Thomas, Ph.D., SDSC HPC Training lead, and Computational Data Scientist in the Data-Enabled Scientific Computing Division. REHS Students: Sahil Samar, Del Norte High School, San Diego, CA, sahilsamar031@gmail.com Mia Chen, Westview High School, San Diego, CA, mialunachen@gmail.com Jack Karpinski, San Diego High School, San Diego, CA, USA, jackadoo4@gmail.com Michael Ray, JSerra Catholic High School, San Juan Capistrano, CA, michael.ray@jserra.org Archita Sarin, Mission San Jose High School, Fremont, CA, archita.sarin@gmail.com Collaborators/Mentors: Christian Garcia, Engineering Scientist Associate (Texas Advanced Computing Center [5]). Matthew Lange, Ph.D., CEO, International Center for Food Ontology Operability Data and Semantics (IC-FOODS [4]); Joe Stubbs, Ph.D., Manager, Cloud & Interactive Computing (Texas Advanced Computing Center [5]).","title":"About the Team"},{"location":"hello_icicle_auth_clients/#acknowledgement","text":"This work has been funded by grants from the National Science Foundation, including: * The AI Institute for Intelligent CyberInfrastructure with Computational Learning in the Environment (ICICLE) ( #2112606 ) * The SDSC Expanse project ( #1928224 ) * The TACC Stampede System ( #1663578 ) * Tapis projects ( #1931439 ) * the NSF Track 3 Award: COre National Ecosystem for CyberinfrasTructure (CONECT) ( #2138307 ) and the Extreme Science and Engineering Discovery Environment (XSEDE) ( ACI-1548562 ). NOTES: YAML file: See Component Data Yaml file: https://github.com/ICICLE-ai/CI-Components-Catalog/blob/master/components-data.yaml","title":"Acknowledgement"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/","text":"Hello ICICLE Authentication Clients Software Release Notes: Software release: icicle_rel_03_2023 Date: 04/14/2023 For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on a TACC server (URL???). The primary goals of these applications included: Authenticating users using TACC accounts Creating Neo4j Pods to be able to store data on Tacc Servers Loading pre-existing data into the Neo4j Pods Setting permissions of Pods Parsing Cypher queries from our clients and directly communicate with the data on our Pods Visualizing data through neo4jupyter (notebooks) Developing an interactive user interface for requesting Cypher input (CLIs) icicle_rel_03_2023 For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on any Tapis service. These applciations are stand-alone and can be installed separately. For details, see: Jupyter Notebooks TapisAuth Example Applications Command Line Interfaces ICICONSOLE TapisCL-ICICLE The Notebooks and the CLIs each have their own directory and software requirements which are described here:","title":"Hello ICICLE Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/#hello-icicle-authentication-clients-software-release-notes","text":"Software release: icicle_rel_03_2023 Date: 04/14/2023 For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on a TACC server (URL???). The primary goals of these applications included: Authenticating users using TACC accounts Creating Neo4j Pods to be able to store data on Tacc Servers Loading pre-existing data into the Neo4j Pods Setting permissions of Pods Parsing Cypher queries from our clients and directly communicate with the data on our Pods Visualizing data through neo4jupyter (notebooks) Developing an interactive user interface for requesting Cypher input (CLIs)","title":"Hello ICICLE Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/#icicle_rel_03_2023","text":"For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on any Tapis service. These applciations are stand-alone and can be installed separately. For details, see: Jupyter Notebooks TapisAuth Example Applications Command Line Interfaces ICICONSOLE TapisCL-ICICLE The Notebooks and the CLIs each have their own directory and software requirements which are described here:","title":"icicle_rel_03_2023"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/","text":"Hello Icicle Authentication Clients Software Release Notes: Command Line Interface applications Software release: icicle_rel_03_2023 Date: 04/14/2023 Overview These two command line applications provide a user friendly way to access and manage Tapis services. General Requirements An account and login credentials for a Tapis service running on HPC system (TACC or Expanse for instance) A python installation TapisCL-ICICLE Functions: * Command line interface to manage and operate Tapis services * Pods * Services * Files * Apps/Jobs Dependencies: here Installation Options: 1. As PyPi package 2. Directly from github see here for installation instructions ICICONSOLE Functions: * Command line interface specifically aimed toward working with Neo4j hosted on Tapis pods. * Direct entry of cypher commands into the console to be executed by Neo4j * Also provides python library which wraps over Cypher commands Dependencies: here Installation Options: 1. As PyPi package 2. Directly from github see here for installation instructions","title":"Hello Icicle Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#hello-icicle-authentication-clients-software-release-notes","text":"","title":"Hello Icicle Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#command-line-interface-applications","text":"Software release: icicle_rel_03_2023 Date: 04/14/2023","title":"Command Line Interface applications"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#overview","text":"These two command line applications provide a user friendly way to access and manage Tapis services.","title":"Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#general-requirements","text":"An account and login credentials for a Tapis service running on HPC system (TACC or Expanse for instance) A python installation","title":"General Requirements"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#tapiscl-icicle","text":"Functions: * Command line interface to manage and operate Tapis services * Pods * Services * Files * Apps/Jobs Dependencies: here Installation Options: 1. As PyPi package 2. Directly from github see here for installation instructions","title":"TapisCL-ICICLE"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#iciconsole","text":"Functions: * Command line interface specifically aimed toward working with Neo4j hosted on Tapis pods. * Direct entry of cypher commands into the console to be executed by Neo4j * Also provides python library which wraps over Cypher commands Dependencies: here Installation Options: 1. As PyPi package 2. Directly from github see here for installation instructions","title":"ICICONSOLE"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/","text":"ICICONSOLE Overview ICICONSOLE is designed to provide an efficient and powerful interface to Neo4j Knowledge Graph databases hosted on HPC resources, leveraging Tapis. This application is specialized for knowledge graph querying, and has some basic CYPHER commands built in. Installation Right now, the only way to use this application is to directly run the python code. Additionally, it requies python 3.10. git clone https://github.com/icicle-ai/hello_icicle_auth_clients.git cd hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE Install dependencies. pip install pandas pip install py2neo pip install tapipy Run ICICONSOLE. python ICICONSOLE.py First time user guide You will be asked to login with your TACC account. If you aren't sure if you have this, visit the TACC portal . Next, you will see the Tapis Pods that you have been given permission to access. If you don't see any, please contact the owner of the Pod you wish to access. Type in the ID of the Pod that you want to access. Once you do this, you will be in a custom made console for interfacing with the Knowledge Graph, using the Cypher language. If you know Cypher, you can start typing in commands like MATCH(n) RETURN n LIMIT 10 If you are not familiar with Cypher, don't worry! This is meant for users who have never used Cypher before. Type in \"help\" to view some of the built in commands to start exploring the knowledge graph. These built in commands will grow more extensive as time goes on. The welcome message for the Knowledge Graph console contains helpful tips, like \"new\", \"exit\", \"clear\", and \"help\".","title":"ICICONSOLE"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/#iciconsole","text":"","title":"ICICONSOLE"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/#overview","text":"ICICONSOLE is designed to provide an efficient and powerful interface to Neo4j Knowledge Graph databases hosted on HPC resources, leveraging Tapis. This application is specialized for knowledge graph querying, and has some basic CYPHER commands built in.","title":"Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/#installation","text":"Right now, the only way to use this application is to directly run the python code. Additionally, it requies python 3.10. git clone https://github.com/icicle-ai/hello_icicle_auth_clients.git cd hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE Install dependencies. pip install pandas pip install py2neo pip install tapipy Run ICICONSOLE. python ICICONSOLE.py","title":"Installation"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/#first-time-user-guide","text":"You will be asked to login with your TACC account. If you aren't sure if you have this, visit the TACC portal . Next, you will see the Tapis Pods that you have been given permission to access. If you don't see any, please contact the owner of the Pod you wish to access. Type in the ID of the Pod that you want to access. Once you do this, you will be in a custom made console for interfacing with the Knowledge Graph, using the Cypher language. If you know Cypher, you can start typing in commands like MATCH(n) RETURN n LIMIT 10 If you are not familiar with Cypher, don't worry! This is meant for users who have never used Cypher before. Type in \"help\" to view some of the built in commands to start exploring the knowledge graph. These built in commands will grow more extensive as time goes on. The welcome message for the Knowledge Graph console contains helpful tips, like \"new\", \"exit\", \"clear\", and \"help\".","title":"First time user guide"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/","text":"TapisCLI Please remember to create an issue in this repository if you encounter any bugs, we will do our best to fix it quick! Overview Tapis CLI is designed to provide a simple to use, versatile way to interface with Tapis services hosted on HPC resources. User can either start the app and use it as a traditional command line applications, or pass commands directly from bash. Allows you to work with all major Tapis services: Pods, Systems, Files, and Apps in one place. It can also interface directly with services being hosted on Tapis pods, like Neo4j. Although currently Neo4j is the only 3rd party application it can work with, adding support for Postgres and the like will not be difficult. Dependencies Dependencies are listed here Installation Using PyPi pip install TapisCL-ICICLE . Current version 0.0.24 python -m TapisCLICICLE Running Python Code Directly Clone the repository to local machine. python -m pip install -r requirements.txt python cli.py Operations Full Terminal Interface: 1. run python -m TapisCLICICLE 2. You will be promted to enter a Tapis service link. You can find this on the Tapis service provider's wesbite usually. If you are working with icicle, this should be https://icicle.tapis.io 3. enter your username and password when prompted 4. if all went well the console should open. You can run help to see command options 5. to exit the application, run exit Command Line: Alternatively, if you do not want to enter the actual command line environment of the TapisCL-ICICLE application, you can run commands directly from the command line like this: python -m TapisCLICICLE pods -c help this may still ask you for authentication, however once you are logged in once, you do not need to enter your credentials again unless the 5 minute timeout period passes, in which case the application shuts itself off. Scripting Python and Bash scripting examples are available here","title":"TapisCLI"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#tapiscli","text":"Please remember to create an issue in this repository if you encounter any bugs, we will do our best to fix it quick!","title":"TapisCLI"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#overview","text":"Tapis CLI is designed to provide a simple to use, versatile way to interface with Tapis services hosted on HPC resources. User can either start the app and use it as a traditional command line applications, or pass commands directly from bash. Allows you to work with all major Tapis services: Pods, Systems, Files, and Apps in one place. It can also interface directly with services being hosted on Tapis pods, like Neo4j. Although currently Neo4j is the only 3rd party application it can work with, adding support for Postgres and the like will not be difficult.","title":"Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#dependencies","text":"Dependencies are listed here","title":"Dependencies"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#installation","text":"","title":"Installation"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#using-pypi","text":"pip install TapisCL-ICICLE . Current version 0.0.24 python -m TapisCLICICLE","title":"Using PyPi"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#running-python-code-directly","text":"Clone the repository to local machine. python -m pip install -r requirements.txt python cli.py","title":"Running Python Code Directly"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#operations","text":"Full Terminal Interface: 1. run python -m TapisCLICICLE 2. You will be promted to enter a Tapis service link. You can find this on the Tapis service provider's wesbite usually. If you are working with icicle, this should be https://icicle.tapis.io 3. enter your username and password when prompted 4. if all went well the console should open. You can run help to see command options 5. to exit the application, run exit Command Line: Alternatively, if you do not want to enter the actual command line environment of the TapisCL-ICICLE application, you can run commands directly from the command line like this: python -m TapisCLICICLE pods -c help this may still ask you for authentication, however once you are logged in once, you do not need to enter your credentials again unless the 5 minute timeout period passes, in which case the application shuts itself off. Scripting Python and Bash scripting examples are available here","title":"Operations"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/Scripting-Examples/","text":"Scripting examples Overview TapisCLICICLE's can be scripted to automatically execute tasks for managing Tapis services. You can do this using: Bash Script: execute a collection of TapisCLICICLE commands from one file Jupyter Notebook/Python: By importing the TapisCLICICLE's constituent packages, most notably the tapisObjectWrappers (which governs the CLI interface with the Tapis API through the TACC Tapis python library, and adds additional function to streamline operations), you can also write python scripts to automate CLI functions.","title":"Scripting examples"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/Scripting-Examples/#scripting-examples","text":"","title":"Scripting examples"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/Scripting-Examples/#overview","text":"TapisCLICICLE's can be scripted to automatically execute tasks for managing Tapis services. You can do this using: Bash Script: execute a collection of TapisCLICICLE commands from one file Jupyter Notebook/Python: By importing the TapisCLICICLE's constituent packages, most notably the tapisObjectWrappers (which governs the CLI interface with the Tapis API through the TACC Tapis python library, and adds additional function to streamline operations), you can also write python scripts to automate CLI functions.","title":"Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/TapisCLICICLE/","text":"For those interested Design TapisCLICICLE uses a localhost client server model to run the application. While this did introduce some complexites to the code, it has several benefits. 1. The Tapis objects, and all other initialization only needs to happen once when the server is turned on. Initialization takes both time, and resources 2. The user only has to log in once for a 5 minute extendable session, so if they want to run commands directly from a bash environment they do not have to log in for every command 3. The program doesnt have to continuously make new login requests to the Tapis service they are connecting to","title":"Index"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/TapisCLICICLE/#for-those-interested","text":"","title":"For those interested"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/TapisCLICICLE/#_1","text":"","title":""},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/TapisCLICICLE/#design","text":"TapisCLICICLE uses a localhost client server model to run the application. While this did introduce some complexites to the code, it has several benefits. 1. The Tapis objects, and all other initialization only needs to happen once when the server is turned on. Initialization takes both time, and resources 2. The user only has to log in once for a 5 minute extendable session, so if they want to run commands directly from a bash environment they do not have to log in for every command 3. The program doesnt have to continuously make new login requests to the Tapis service they are connecting to","title":"Design"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/tapis-config-files/","text":"Config Files This is where examples for config files go. In order to make full use of the client, the user must load config files for systems and apps, etc.","title":"Config Files"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/tapis-config-files/#config-files","text":"This is where examples for config files go. In order to make full use of the client, the user must load config files for systems and apps, etc.","title":"Config Files"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/","text":"Hello Icicle Authentication Clients Software Release Notes: Secure Notebooks for Accessing Tapis PODs Software release: icicle_rel_03_2023 Date: 04/14/2023 Notebook Overview These notebooks are used as templates for developing Tapis authenticated applications, or as demonstrators/examples of what you can do with Tapis in your applications. TapisAuthTemplate This notebook provides an extensible, easy to use base to build out a Tapis authenticated application. It comes with the Tapis auth included, and allows the user to add code to interface with Tapis without having to worry about the authorization. requirements: here these apply to the other applications below as well Example Notebooks Demonstrators of possible Tapis use cases Pods These notebooks are demonstrators for the Tapis pods service, for now focusing on Neo4j pods which can host knowledge graphs. They also demonstrate usage of the extensible Tapis Auth notebook. load_data Demonstrates several methods of loading data from CSV files into a Neo4j knowledge graph, through Tapis. PPod Demonstrates querying of a Neo4j knowledge graph pod hosting the ICICLE PPod ontology. query_neo4j_architecture Demonstrates querying of a Neo4j knowledge graph pod hosting the Tapis architecture diagram","title":"Hello Icicle Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#hello-icicle-authentication-clients-software-release-notes","text":"Secure Notebooks for Accessing Tapis PODs Software release: icicle_rel_03_2023 Date: 04/14/2023","title":"Hello Icicle Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#notebook-overview","text":"These notebooks are used as templates for developing Tapis authenticated applications, or as demonstrators/examples of what you can do with Tapis in your applications.","title":"Notebook Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#tapisauthtemplate","text":"This notebook provides an extensible, easy to use base to build out a Tapis authenticated application. It comes with the Tapis auth included, and allows the user to add code to interface with Tapis without having to worry about the authorization. requirements: here these apply to the other applications below as well","title":"TapisAuthTemplate"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#example-notebooks","text":"Demonstrators of possible Tapis use cases","title":"Example Notebooks"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#pods","text":"These notebooks are demonstrators for the Tapis pods service, for now focusing on Neo4j pods which can host knowledge graphs. They also demonstrate usage of the extensible Tapis Auth notebook.","title":"Pods"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#load_data","text":"Demonstrates several methods of loading data from CSV files into a Neo4j knowledge graph, through Tapis.","title":"load_data"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#ppod","text":"Demonstrates querying of a Neo4j knowledge graph pod hosting the ICICLE PPod ontology.","title":"PPod"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#query_neo4j_architecture","text":"Demonstrates querying of a Neo4j knowledge graph pod hosting the Tapis architecture diagram","title":"query_neo4j_architecture"},{"location":"hello_icicle_auth_clients/images/readme/","text":"images","title":"Readme"},{"location":"iSpLib/","text":"iSpLib - An Intelligent Sparse Library iSpLib is an accelerated sparse kernel library with PyTorch interface. This library has an auto-tuner which generates optimized custom sparse kernels based on the user environment. The goal of this library is to provide efficient sparse operations for Graph Neural Network implementations. Currently it has support for CPU-based efficient Sparse Dense Matrix Multiplication (spmm-sum only) with autograd. System Requirements Users need to have the following software/tools installed in their PC/server. The source code was compiled and run successfully in Linux (Ubuntu and Centos distributions). Make >= 4.4 GCC >= 10.1 (Support for C++ >= 14) OpenMP >= 4.5 Python >= 3.7 NumPy >= 1.21 (Or, Anaconda environment) Installation To install the package, run the following commands: git clone https://github.com/ICICLE-ai/iSpLib.git : To clone this repository. ./configure : To download and run the auto-tuner. This is a pre-requisite for the installation. Create a virtualenv as the packages might conflict. Install the dependencies pip install torch torchvision scikit-learn torch-scatter . make : To install the library. Finally install custom version of torch-geometric pip install git+https://github.com/gamparohit/pytorch_geometric.git Troubleshoot If make command exits with unknown error message, try running pip3 install -e . instead. If you are having trouble installing torch-scatter, install it with -f flag and torch version. import torch print(torch.__version__) !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html Convention and Usage The current iSpLib has both regular and optimized SpMM operation for comparison purpose. The default operation is non-optimized. To use the optimized version, use the following code snippet for running the GNN model: import builtins builtins.FUSEDMM = True FusedMM method is used as the optimized sparse kernel and it is generated when ./configure command is run. See details here: FusedMM Method . Note: If you are not using the optimized kernel, you will still have to explicitly mention builtins.FUSEDMM = False in the code, otherwise it will raise an error. Performance and Testing When compared to PyTorch Sparse, a 2-layer GCN implemention with 10 epochs is typically- 2.5x faster on Cora dataset 2x faster on Reddit dataset [Note: The speed-up varies largly depending on the system condition] To run the test code, use the command: make test . This runs the python script in tests/GCN.py and prints out the speed-up along with the accuracy. See tests/Expected_GCN_output.txt for reference. SpMM Example The SpMM operation can also be used directly to multiply two compatible matrices. Following is an example of a sparse and dense matrix multiplication: import builtins from isplib.matmul import * from isplib.tensor import SparseTensor from scipy.sparse import coo_matrix import torch index = torch.tensor([[0, 0, 1, 2, 2], [0, 2, 1, 0, 1]]) value = torch.Tensor([1, 2, 4, 1, 3]) matrix = torch.Tensor([[90, 4], [2, 5], [3, 6]]) a = SparseTensor.from_scipy(coo_matrix((value, index), shape=(3, 3))) b = matrix builtins.FUSEDMM = True print(spmm_sum(a, b)) License iSpLib is licensed under the https://opensource.org/licenses/BSD-3-Clause Acknowledgements This work has been funded by grants from the National Science Foundation, including the ICICLE AI Institute (OAC 2112606)","title":"iSpLib - An Intelligent Sparse Library"},{"location":"iSpLib/#isplib-an-intelligent-sparse-library","text":"iSpLib is an accelerated sparse kernel library with PyTorch interface. This library has an auto-tuner which generates optimized custom sparse kernels based on the user environment. The goal of this library is to provide efficient sparse operations for Graph Neural Network implementations. Currently it has support for CPU-based efficient Sparse Dense Matrix Multiplication (spmm-sum only) with autograd.","title":"iSpLib - An Intelligent Sparse Library"},{"location":"iSpLib/#system-requirements","text":"Users need to have the following software/tools installed in their PC/server. The source code was compiled and run successfully in Linux (Ubuntu and Centos distributions). Make >= 4.4 GCC >= 10.1 (Support for C++ >= 14) OpenMP >= 4.5 Python >= 3.7 NumPy >= 1.21 (Or, Anaconda environment)","title":"System Requirements"},{"location":"iSpLib/#installation","text":"To install the package, run the following commands: git clone https://github.com/ICICLE-ai/iSpLib.git : To clone this repository. ./configure : To download and run the auto-tuner. This is a pre-requisite for the installation. Create a virtualenv as the packages might conflict. Install the dependencies pip install torch torchvision scikit-learn torch-scatter . make : To install the library. Finally install custom version of torch-geometric pip install git+https://github.com/gamparohit/pytorch_geometric.git","title":"Installation"},{"location":"iSpLib/#troubleshoot","text":"If make command exits with unknown error message, try running pip3 install -e . instead. If you are having trouble installing torch-scatter, install it with -f flag and torch version. import torch print(torch.__version__) !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html","title":"Troubleshoot"},{"location":"iSpLib/#convention-and-usage","text":"The current iSpLib has both regular and optimized SpMM operation for comparison purpose. The default operation is non-optimized. To use the optimized version, use the following code snippet for running the GNN model: import builtins builtins.FUSEDMM = True FusedMM method is used as the optimized sparse kernel and it is generated when ./configure command is run. See details here: FusedMM Method . Note: If you are not using the optimized kernel, you will still have to explicitly mention builtins.FUSEDMM = False in the code, otherwise it will raise an error.","title":"Convention and Usage"},{"location":"iSpLib/#performance-and-testing","text":"When compared to PyTorch Sparse, a 2-layer GCN implemention with 10 epochs is typically- 2.5x faster on Cora dataset 2x faster on Reddit dataset [Note: The speed-up varies largly depending on the system condition] To run the test code, use the command: make test . This runs the python script in tests/GCN.py and prints out the speed-up along with the accuracy. See tests/Expected_GCN_output.txt for reference.","title":"Performance and Testing"},{"location":"iSpLib/#spmm-example","text":"The SpMM operation can also be used directly to multiply two compatible matrices. Following is an example of a sparse and dense matrix multiplication: import builtins from isplib.matmul import * from isplib.tensor import SparseTensor from scipy.sparse import coo_matrix import torch index = torch.tensor([[0, 0, 1, 2, 2], [0, 2, 1, 0, 1]]) value = torch.Tensor([1, 2, 4, 1, 3]) matrix = torch.Tensor([[90, 4], [2, 5], [3, 6]]) a = SparseTensor.from_scipy(coo_matrix((value, index), shape=(3, 3))) b = matrix builtins.FUSEDMM = True print(spmm_sum(a, b))","title":"SpMM Example"},{"location":"iSpLib/#license","text":"iSpLib is licensed under the https://opensource.org/licenses/BSD-3-Clause","title":"License"},{"location":"iSpLib/#acknowledgements","text":"This work has been funded by grants from the National Science Foundation, including the ICICLE AI Institute (OAC 2112606)","title":"Acknowledgements"},{"location":"yamltest/","text":"Hello","title":"Hello"},{"location":"yamltest/#hello","text":"","title":"Hello"}]}